{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##greedy-fragment-partitioner\n",
    "\n",
    "Partitions reads greedily based on hapcut blocks and adds the information to a BAM file, also produces a statistics file of blocks and phasable and unphasable reads. Does some other things too like partitioning based on the truth trio data (see code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run ./ipy_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ./ipy_setup.py\n",
    "# Imports / style (run this first always)\n",
    "\n",
    "# %matplotlib inline ## magic functions don't work here!\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "class AwesomeError(Exception):\n",
    "     def __init__(self, value):\n",
    "         self.value = value\n",
    "         pass\n",
    "     def __str__(self):\n",
    "         return repr(self.value)\n",
    "         pass\n",
    "\n",
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
    "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
    "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
    "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
    "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
    "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
    "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 200\n",
    "rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "\n",
    "\n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import Bio as bp\n",
    "from Bio.Sequencing.Applications import BwaAlignCommandline as bwa_aln\n",
    "from Bio.Sequencing.Applications import BwaSamseCommandline as bwa_samse\n",
    "from Bio.Sequencing.Applications import BwaSampeCommandline as bwa_sampe\n",
    "from Bio.Sequencing.Applications import BwaIndexCommandline as bwa_index\n",
    "from Bio.Sequencing.Applications import BwaBwaswCommandline as bwa_bwasw\n",
    "import HTSeq as ht\n",
    "import subprocess\n",
    "\n",
    "# progress bar\n",
    "\n",
    "import sys, time\n",
    "try:\n",
    "    from IPython.display import clear_output\n",
    "    have_ipython = True\n",
    "except ImportError:\n",
    "    have_ipython = False\n",
    "\n",
    "class ProgressBar:\n",
    "    def __init__(self, iterations):\n",
    "        self.iterations = iterations\n",
    "        self.start = time.time()\n",
    "        self.time = time.time()\n",
    "        self.count = 0\n",
    "        self.prog_bar = '[]'\n",
    "        self.fill_char = '*'\n",
    "        self.width = 40\n",
    "        self.__update_amount(0)\n",
    "        if have_ipython:\n",
    "            self.animate = self.animate_ipython\n",
    "        else:\n",
    "            self.animate = None\n",
    "\n",
    "    def animate_ipython(self):\n",
    "        self.count += 1\n",
    "        if round(time.time() - self.time) > 1: # throttling to 1 second\n",
    "            print '\\r', self,\n",
    "            sys.stdout.flush()\n",
    "            self.update_iteration(self.count)\n",
    "            self.time = time.time()\n",
    "\n",
    "    def update_iteration(self, elapsed_iter):\n",
    "        elapsed = time.time()-self.start # in seconds\n",
    "        time_remaining = (self.iterations - elapsed_iter)*elapsed/float(elapsed_iter)\n",
    "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
    "        self.prog_bar += '  %d of %s complete' % (elapsed_iter, self.iterations)\n",
    "        self.prog_bar += ' Elapsed: %s minutes\\t Remaining: %s minutes' % (round(elapsed/60.,1), round(time_remaining/60.,1))\n",
    "\n",
    "    def __update_amount(self, new_amount):\n",
    "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
    "        all_full = self.width - 2\n",
    "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
    "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
    "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
    "        pct_string = '%d%%' % percent_done\n",
    "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
    "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.prog_bar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# imports\n",
    "import os, sys, getopt\n",
    "import pysam\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "global count\n",
    "count = 0\n",
    "\n",
    "# init main\n",
    "def main(argv):\n",
    "    hairs_file = ''\n",
    "    hapcut_file = ''\n",
    "    bam_file = ''\n",
    "    out_file = ''\n",
    "    help = 'greedy_partitioner.py -h <input.hairs> -c <input.hapcut> -i <input.bam> -o <output.ann.bam>'\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"h:c:i:o:\",[\"hairs=\",\"hapcut=\", \"input=\", \"output=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print help\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '--help':\n",
    "            print help\n",
    "            sys.exit()\n",
    "        elif opt in (\"-h\", \"--hairs\"):\n",
    "            hairs_file = arg\n",
    "        elif opt in (\"-c\", \"--hapcut\"):\n",
    "            hapcut_file = arg\n",
    "        elif opt in (\"-i\", \"--input\"):\n",
    "            bam_file = arg\n",
    "        elif opt in (\"-o\", \"--output\"):\n",
    "            out_file = arg\n",
    "        else:\n",
    "            assert False, \"unhandled option\"\n",
    "\n",
    "    assert pysam.Samfile(bam_file, 'rb'), 'ERROR: Cannot open bam file for reading.'\n",
    "    assert open(bam_file + '.bai', 'rb'), 'ERROR: bam file is not indexed!'\n",
    "    bam_fp = pysam.Samfile(bam_file, 'rb')\n",
    "\n",
    "    if out_file==None:\n",
    "        out_file = bam_file + \".ann_haplotypes_\" + time.strftime(\"%m%d%y_%H%M%S\") + '.bam'\n",
    "\n",
    "    assert pysam.AlignmentFile(out_file, \"wb\", template=bam_fp), 'ERROR: Cannot open output file for writing.'\n",
    "    out_fp = pysam.AlignmentFile(out_file, \"wb\", template=bam_fp)\n",
    "\n",
    "    assert open(hairs_file), 'ERROR: Cannot access hairs file.'\n",
    "    assert open(hapcut_file), 'ERROR: Cannot open hapcut file.'\n",
    "\n",
    "    hair_reader = HairReader(hairs_file)\n",
    "    block_reader = HapCutReader(hapcut_file)\n",
    "    stats_file = out_file + \".interblock_stats.tsv\"\n",
    "    sys.stdout.write(\"Loaded greedy_partitoner.py, beginning execution. \\n\")\n",
    "    sys.stdout.write(\"Placing hairfile reads into haplotype blocks. \\n\")\n",
    "    sys.stdout.flush()\n",
    "    hair_reader.assemble_blocks(block_reader)\n",
    "    sys.stdout.write(\"Placing bamfile reads into haplotype blocks. \\n\")\n",
    "    sys.stdout.flush()\n",
    "    unphased_reads = unphased_heuristics(hair_reader, block_reader, bam_fp)\n",
    "    sys.stdout.write(\"Partitioning reads. \\n\")\n",
    "    sys.stdout.flush()\n",
    "    tag_reads(bam_fp, hair_reader, block_reader, unphased_reads, out_fp) #begin tagging reads\n",
    "    sys.stdout.write(\"Calculating interblock statistics. \\n\")\n",
    "    sys.stdout.flush()\n",
    "    interblock_stats(hair_reader, block_reader, stats_file, bam_fp) #generate interblock stats\n",
    "    bam_fp.close()\n",
    "    out_fp.close()\n",
    "\n",
    "# end of main\n",
    "\n",
    "### CLASSES ###\n",
    "\n",
    "class BlockVariant:\n",
    "    def __init__ (self, variantline):\n",
    "        # variant_id haplotype_1 haplotype_2 chromosome position refallele variantallele genotype allele_counts:genotype_likelihoods:delta:MEC_variant\n",
    "        ll = variantline.strip().split(\"\\t\")\n",
    "        var_id, hap1, hap2, chrom, pos, r_allele, v_allele, genotype, info_str = ll\n",
    "        self.chrom, self.r_allele, self.v_allele, self.info_str = chrom, r_allele, v_allele, info_str\n",
    "        self.var_id, self.hap1, self.hap2, self.pos = int(var_id), hap1, hap2, int(pos)\n",
    "        allele_counts, genotype_likelihoods, delta, MEC_variant = info_str.split(\":\")[0:4]\n",
    "        self.ref_count, self.alt_count = map(int, allele_counts.split(\",\"))\n",
    "        gen_00, gen_01, gen_11 = map(float, genotype_likelihoods.split(\",\"))\n",
    "        self.gen_like = {\"0/0\":gen_00, \"0/1\":gen_01, \"1/1\":gen_11}\n",
    "        self.delta = float(delta)\n",
    "        self.MEC_variant = MEC_variant\n",
    "    def __repr__ (self):\n",
    "        return \"<BlockVariant, var_id: %s>\" % str(self.var_id)\n",
    "\n",
    "\n",
    "class Block: # part of block reader\n",
    "    def __init__ (self, blockline):\n",
    "        # \"BLOCK: offset:\" first_variant_block \"len:\" length_of_block \"phased\": phased_variants_block SPAN: \n",
    "        # lengthspanned MECscore score fragments #fragments\n",
    "\n",
    "        ll               = blockline.strip().split()\n",
    "        self.offset      = int(ll[2])\n",
    "        self.total_len   = int(ll[4])\n",
    "        self.phased      = int(ll[6])\n",
    "        self.span        = int(ll[8])\n",
    "        self.MECscore    = float(ll[10])\n",
    "        self.fragments   = int(ll[12])\n",
    "        self.variants \t = dict() # default to empty\n",
    "        self.variant_ids = set()\n",
    "        self.chrom = None\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.informative_reads = []\n",
    "        self.unphased_reads = []\n",
    "        self.unphased_read_set = set()\n",
    "        self.read_count = 0\n",
    "        self.read_set = set()\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<Block, offset_id: %s>\" % str(self.offset)\n",
    "\n",
    "    def addVariant(self, variantline):\n",
    "        variant = BlockVariant(variantline)\n",
    "        self.variants[variant.var_id] = variant\n",
    "        self.variant_ids.add(variant.var_id)\n",
    "        self.updatePosition(variant)\n",
    "\n",
    "    def updatePosition(self, variant): # we need to do this because sometimes the variant isn't associated with a block\n",
    "        if (self.chrom != None) & (self.start != None) & (self.end != None):\n",
    "            if variant.pos < self.start:\n",
    "                self.start = variant.pos\n",
    "            elif variant.pos > self.end:\n",
    "                self.end = variant.pos\n",
    "            if self.chrom != variant.chrom:\n",
    "                sys.stderr.write(\"WARNING: Cannot add variants from other contigs to current block.\")\n",
    "                return 1\n",
    "        else:\n",
    "            self.chrom = variant.chrom\n",
    "            self.start = variant.pos\n",
    "            self.end = variant.pos\n",
    "\n",
    "    def addReadsToBlock(self, read_dict):\n",
    "        self.informative_reads = []\n",
    "        for k,read in read_dict.iteritems():\n",
    "            read_ids = frozenset([var[0] for block in read.blocks for var in block])\n",
    "            if len(set(read_ids).intersection(set(self.variant_ids))) > 0:\n",
    "                self.informative_reads.append(read)\n",
    "        self.read_count = len(self.informative_reads)\n",
    "        self.read_set = frozenset([x.read_id for x in self.informative_reads])\n",
    "\n",
    "    def concordance(self, input_reads):\n",
    "        ''' this should return a dict of (#T,#F) tuples per variant\n",
    "         each element is a variant's concordance with the reads\n",
    "         using the read's haplotype information, we can establish whether the read's phasing\n",
    "         is consistent with how the variant was phased '''\n",
    "        variant_concord = dict()\n",
    "        support_reads_hap2 = 0\n",
    "        against_reads_hap2 = 0\n",
    "        support_reads_hap1 = 0\n",
    "        against_reads_hap1 = 0\n",
    "        for k,variant in self.variants.iteritems():\n",
    "            for read in input_reads:\n",
    "                if variant.var_id in read.positions:\n",
    "                    read_allele = read.alleles[read.positions.index(variant.var_id)]\n",
    "                    hapstate = read.haplotypes[self.offset]\n",
    "                    if hapstate == 2:\n",
    "                        if variant.hap2 != read_allele:\n",
    "                            against_reads_hap2 += 1\n",
    "                        else:\n",
    "                            support_reads_hap2 += 1\n",
    "                    else:\n",
    "                        if variant.hap1 != read_allele:\n",
    "                            against_reads_hap1 += 1\n",
    "                        else:\n",
    "                            support_reads_hap1 += 1\n",
    "        variant_concord[self.offset] = {\"hap1\": (support_reads_hap1, against_reads_hap1), \n",
    "                                               \"hap2\": (support_reads_hap2, against_reads_hap2)}\n",
    "        return variant_concord\n",
    "\n",
    "    def variant(self, var_id):\n",
    "        try:\n",
    "            return self.variants[var_id]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def interblock_reads(self, input_reads):\n",
    "        out_reads = []\n",
    "        for read in input_reads:\n",
    "            if read.read_id not in self.read_set:\n",
    "                if read.chrom == self.chrom:\n",
    "                    if ((read.start < self.end) & (read.end > self.end)) | \\\n",
    "                        ((read.end > self.start) & (read.start < self.start)) | \\\n",
    "                        ((read.end <= self.end) & (read.start >= self.start)):\n",
    "                        out_reads.append(read)\n",
    "        return out_reads\n",
    "    \n",
    "    def add_read(self, read_obj):\n",
    "        if read_obj.read_id not in self.read_set:\n",
    "            self.informative_reads.append(read_obj)\n",
    "            self.read_set.add(read_obj.read_id)\n",
    "    \n",
    "    def add_unphased_reads(self, bam_fp):\n",
    "        reads = bam_fp.fetch(region=self.chrom + ':' + str(self.start) + '-' + str(self.end))\n",
    "        for read in reads:\n",
    "            if read.query_name not in self.read_set:\n",
    "                self.unphased_reads.append(read)\n",
    "                self.unphased_read_set.add(read.query_name)\n",
    "        return None\n",
    "\n",
    "class HapCutReader: # hapcut file reader\n",
    "\n",
    "    def __init__ ( self, fn ):\n",
    "        self.fn = fn\n",
    "        self.blocks = dict()\n",
    "        self.translate = dict()\n",
    "        for block in self.read_file_to_blocks(fn):\n",
    "            self.blocks[block.offset] = block\n",
    "            for v in block.variant_ids: # v is an id\n",
    "                self.translate[v] = block.offset\n",
    "\n",
    "    def loc(self, block_id):\n",
    "        try:\n",
    "            return self.blocks[self.translate[block_id]]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def read_file_to_blocks(self, fn):\n",
    "        with open(fn) as f:\n",
    "            currBlock = None\n",
    "            for l in f:\n",
    "                if l[0] == \"B\": # starting a new block\n",
    "                    currBlock = Block(l)\n",
    "                elif l[0] == \"*\": # ending a block\n",
    "                    yield currBlock\n",
    "                else:\n",
    "                    currBlock.addVariant(l)\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<HapCutReader, filename: %s>\" % self.fn\n",
    "    \n",
    "    def assemble_reads(self, hair_reader):\n",
    "        for k,block in self.blocks.iteritems():\n",
    "            block.addReadsToBlock(hair_reader.reads)\n",
    "\n",
    "class HapCutRead: #hair file line\n",
    "\n",
    "    def __init__ (self, hairline):\n",
    "        #Column 1 is the number of consecutive set of SNPs covered by the fragment, NOT haplotype blocks.\n",
    "        #Column 2 is the fragment id. \n",
    "        #Column 3 is the offset of the first block of SNPs covered by the fragment followed by the alleles at the SNPs in this block.\n",
    "        #Column 5 is the offset of the second block of SNPs covered by the fragment followed by the alleles at the SNPs in this block.\n",
    "        #...\n",
    "        #The last column is a string with the quality values (Sanger fastq format) for all the alleles covered by the fragment (concatenated for all blocks). \n",
    "        #For example, if a read/fragment covers SNPs 2,3 and 5 with the alleles 0, 1 and 0 respectively, then the input will be:\n",
    "        #2 read_id 2 01 5 0 AAC\n",
    "        #Here AAC is the string corresponding to the quality values at the three alleles. The encoding of 0/1 is arbitrary but following the VCF format, 0 is reference and 1 is alternate. \n",
    "        hairlist = hairline.strip().split()\n",
    "        self.blockcount = 0                # this information must be determined afterwards \n",
    "        self.read_id    = hairlist[1]      # read_id\n",
    "        self.blocks     = []\t\t       # an array of tuples corresponding to blocks\n",
    "        self.positions  = []\n",
    "        self.alleles    = []\n",
    "        self.chrom      = None\n",
    "        self.start      = None\n",
    "        self.end        = None\n",
    "        self.haplotypes = dict()             # an array of {\"block_offset\":\"haplotype\"} \n",
    "                                           # after partitioning\n",
    "        for i in range(2, len(hairlist)-1, 2):\n",
    "            position = int(hairlist[i])\n",
    "            allele = hairlist[i+1]\n",
    "            block = zip(range(position, position+len(allele)), allele)\n",
    "            self.blocks.append(block)\n",
    "            self.positions.extend(range(position, position+len(allele)))\n",
    "            self.alleles.extend(allele)\n",
    "            self.qualities  = hairlist[-1]         # a matched arary of the qualities of allele calls\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<HapCutRead, read_id: %s>\" % str(self.read_id)\n",
    "\n",
    "    def haplotype_fields(self):\n",
    "        haps = \";\".join([','.join([str(key), str(self.haplotypes[key])]) for key in self.haplotypes])\n",
    "        haptag = [(\"ZH\", haps), (\"ZB\", int(self.blockcount)), (\"ZV\", len(self.positions))]\n",
    "        return haptag\n",
    "    \n",
    "    def addGenomicPositions(self, block_reader):\n",
    "        arr = []\n",
    "        chrom = None\n",
    "        for position in self.positions:\n",
    "            b = block_reader.loc(position)\n",
    "            if b == None:\n",
    "                continue\n",
    "            if chrom == None:\n",
    "                chrom = b.chrom\n",
    "            arr.append(b.variant(position).pos)\n",
    "        if len(arr) > 0:\n",
    "            self.chrom = chrom\n",
    "            self.start = np.min(arr)\n",
    "            self.end = np.max(arr)\n",
    "        else:\n",
    "            self.chrom = '*'\n",
    "            self.start = None\n",
    "            self.end = None\n",
    "    \n",
    "    def assemble_blocks(self, block_reader):\n",
    "        # it turns out that the blocks provided in a hapcut file don't actually correspond to real blocks\n",
    "        # just contiguous alleles?\n",
    "        self.blocks = []\n",
    "        lastBlock = -1\n",
    "        for ix, pos in enumerate(self.positions): # self.positions corresponds to a variant id\n",
    "            currBlock = block_reader.loc(pos) # look up block associated with variant\n",
    "            if currBlock == None: # if it's not, continue\n",
    "                continue\n",
    "            # let's also add ourselves to the block\n",
    "            currBlock.add_read(self) #if we get a block back, add the read to the block's read set\n",
    "            currBlock = currBlock.offset # set our read's block id\n",
    "            if currBlock != lastBlock:\n",
    "                self.blocks.append([])\n",
    "            self.blocks[-1].append((pos, self.alleles[ix])) # a read can be more than one haplotype block long\n",
    "            lastBlock = currBlock\n",
    "        self.blockcount = len(self.blocks) # determine number of haplotype blocks read spans\n",
    "        self.addGenomicPositions(block_reader) # determine start-end positions of read\n",
    "    \n",
    "class HairReader:\n",
    "\n",
    "    def __init__ (self, fn):\n",
    "        self.fn = fn\n",
    "        self.reads = dict()\n",
    "        with open (fn) as f:\n",
    "            for l in f:\n",
    "                newread = HapCutRead(l)\n",
    "                self.reads[newread.read_id] = newread\n",
    "        self.read_set = frozenset(self.reads.keys())\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<HairReader, filename: %s>\" % self.fn\n",
    "\n",
    "    def loc(self, read_id):\n",
    "        try:\n",
    "            return self.reads[read_id]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def assemble_blocks(self, block_reader):\n",
    "        for k,read in self.reads.iteritems():\n",
    "            read.assemble_blocks(block_reader)\n",
    "\n",
    "### FUNCTIONS ###\n",
    "\n",
    "'''\n",
    "\n",
    "tag_reads()\n",
    "\n",
    "Usage: Tags reads from a bam file corresponding to a particular haplotype, with haplotype\n",
    "definitions from HapCut, under the optional tag \"ZH\".\n",
    "\n",
    "Inputs:\n",
    "    bam_fp\n",
    "    A pysam.Samfile object pointing to the input file\n",
    "    hair_reader\n",
    "    A HairReader object pointing to the hairs file.\n",
    "    block_reader\n",
    "    A HapcutReader object pointing to the hapcut file.\n",
    "    out_fp\n",
    "    A pysam.AlignmentFile pointing to the output bam.\n",
    "Outputs:\n",
    "    (none - writes to out_fp)\n",
    "\n",
    "'''\n",
    "\n",
    "def tag_reads(bam_fp, hair_reader, block_reader, unphased_reads, out_fp):\n",
    "    ''' tag_reads(bam_fp, hair_reader, block_reader, out_fp)'''\n",
    "    global count\n",
    "    for bamread in bam_fp.fetch(region=\"000000F\", start = 10279323, end=10409440):\n",
    "        count += 1\n",
    "        if (count % 100) == 0:\n",
    "            sys.stdout.write(\"\\rWritten %s lines to output.\" % str(count))\n",
    "            sys.stdout.flush()\n",
    "        if bamread.query_name in hair_reader.read_set:\n",
    "            read = hair_reader.loc(bamread.query_name)\n",
    "            read = greedy_partition(read, block_reader)\n",
    "            bamread.tags += read.haplotype_fields()      # add the haplotype information to hairfile read\n",
    "            out_fp.write(bamread)\n",
    "        elif bamread.query_name in unphased_reads:\n",
    "            read = unphased_reads[bamread.query_name]\n",
    "            read = greedy_partition(read, block_reader)\n",
    "            bamread.tags += read.haplotype_fields()    # add the haplotype information to bam-based read\n",
    "            out_fp.write(bamread)\n",
    "        #out_fp.write(bamread) # write read to file\n",
    "\n",
    "'''\n",
    "greedy_partition()\n",
    "Ryan Neff\n",
    "\n",
    "inputs:\n",
    "read\n",
    "    a HapCutRead object\n",
    "block_reader\n",
    "    of the type HapCutReader\n",
    "\n",
    "outputs:\n",
    "    the original read, now with haplotype information.\n",
    "\n",
    "translate hairfile alleles into blockvar IDs\n",
    "get alleles in each read spanning blockvars\n",
    "determine alleles for the two blocks from blockvar\n",
    "partition read based on locally most probable alignment\n",
    "\n",
    "'''\n",
    "\n",
    "def greedy_partition(read, block_reader):\n",
    "    for readblock in read.blocks:\n",
    "        positions = [x[0] for x in readblock]\n",
    "        alleles = [x[1] for x in readblock]\n",
    "        allele_state = []\n",
    "        offset = positions[0]\n",
    "        hap = 0\n",
    "        block = block_reader.loc(offset) #retrieve block the read is in\n",
    "        if block == None: # this happens when hapcut throws out the block the read is in \n",
    "            continue\n",
    "        offset = block.offset\n",
    "        for ix, varpos in enumerate(positions):\n",
    "            blockvar = block.variant(varpos) # retrieve variant from VCF file\n",
    "            if blockvar.hap1 == alleles[ix]:\n",
    "                allele_state.append(-1)\n",
    "            elif blockvar.hap2 == alleles[ix]:\n",
    "                allele_state.append(1)\n",
    "            else:\n",
    "                #sys.stderr.write(\"\\nWarning: read allele matched no haplotypes.\")\n",
    "                #sys.stderr.write(\"\\nHair read: %s\" % read.read_id)\n",
    "                #sys.stderr.write(\"\\nAlleles: %s\" % str(alleles[ix]))\n",
    "                #sys.stderr.write(\"\\n Hap 1: %s, Hap 2: %s\\n\" % (blockvar.hap1, blockvar.hap2))\n",
    "                #sys.stderr.flush()\n",
    "                continue\n",
    "        if len(allele_state) < 1:\n",
    "            sys.stderr.write(\"Warning: no haplotype information in read.\\n\")\n",
    "            sys.stderr.write(\"\\nHair read: %s\" % read.read_id)\n",
    "            sys.stderr.write(\"\\nAlleles: %s\\n\" % str(alleles))\n",
    "            sys.stderr.flush()\n",
    "            hap = -1\n",
    "        print sum(allele_state), len(allele_state)\n",
    "        if sum(allele_state) < 0:\n",
    "            hap = 1\n",
    "        elif sum(allele_state) > 0:\n",
    "            hap = 2\n",
    "        else:\n",
    "            hap = 0\n",
    "        read.haplotypes[offset] = hap\n",
    "    return read\n",
    "\n",
    "'''\n",
    "interblock_stats()\n",
    "\n",
    "Usage: Creates a tab-separated values file with statistics about reads overlapping\n",
    "between nearby blocks, and finds the concordance of these interblock reads\n",
    "with haplotypes in other blocks. \n",
    "\n",
    "inputs:\n",
    "    hair_reader\n",
    "        A HairReader object\n",
    "    block_reader\n",
    "        A HapcutReader object\n",
    "    out_stats\n",
    "        A string where the .tsv should be written. Defaults\n",
    "        to the hairs filename given in the input + 'interblock_stats.tsv'\n",
    "outputs:\n",
    "    none-writes to file directly\n",
    "\n",
    "'''\n",
    "\n",
    "def interblock_stats(hair_reader, block_reader, out_stats, bam_fp):\n",
    "    blockdist = []\n",
    "    lastChr = None\n",
    "    lastPos = None\n",
    "    lastBlock = None\n",
    "    lastReads = set()\n",
    "    for k,read in hair_reader.reads.iteritems():\n",
    "        if read.haplotypes == dict():\n",
    "            read = greedy_partition(read, block_reader)\n",
    "    for ix, key in enumerate(sorted(block_reader.blocks.keys())):\n",
    "        sys.stdout.write('\\r%s percent done.' % round(ix/float(len(block_reader.blocks))*100))\n",
    "        sys.stdout.flush()\n",
    "        block = block_reader.blocks[key]\n",
    "        if block.read_set == set():\n",
    "            block.addReadsToBlock(hair_reader.reads)\n",
    "        currBlock = block.offset\n",
    "        currChr = block.chrom\n",
    "        currPos = block.start\n",
    "        if lastBlock != None:\n",
    "            if lastChr == currChr:\n",
    "                interblock_reads = block.interblock_reads(lastBlock_obj.informative_reads)\n",
    "                row=[lastBlock, currBlock, currChr, lastPos, currPos, currPos-lastPos,\n",
    "                     lastBlock_obj.end-lastBlock_obj.start, block.end-block.start,\n",
    "                     len(lastBlock_obj.variant_ids), len(block.variant_ids), \n",
    "                     len(lastBlock_obj.read_set), len(block.read_set),\n",
    "                     len(list(bam_fp.fetch(region=lastChr + ':' + str(lastBlock_obj.start) + '-' + str(lastBlock_obj.end)))),\n",
    "                     len(list(bam_fp.fetch(region=lastChr + ':' + str(block.start) + '-' + str(block.end)))),\n",
    "                     len(interblock_reads),\n",
    "                     len(list(bam_fp.fetch(region=lastChr + ':' + str(lastBlock_obj.end) + '-' + str(block.start))))]\n",
    "                     #lastBlock_obj.concordance(lastBlock_obj.informative_reads), \n",
    "                     #block.concordance(block.informative_reads)]\n",
    "                blockdist.append(row)\n",
    "            else:\n",
    "                continue\n",
    "        lastBlock = currBlock\n",
    "        lastBlock_obj = block\n",
    "        lastChr = currChr\n",
    "        lastPos = block.end\n",
    "    header = ['block1', 'block2', 'chrom', 'block1_end', 'block2_start', \n",
    "              'distance', 'block1_size', 'block2_size', 'block1_variants', 'block2_variants', \n",
    "              'block1_informative_reads', 'block2_informative_reads', \n",
    "              'block1_reads', 'block2_reads',\n",
    "              'informative_interblock_reads', \n",
    "              'all_interblock_reads'] \n",
    "              #'block1_concordance', 'block2_concordance']\n",
    "    info = pd.DataFrame(blockdist, columns=header)\n",
    "    info.to_csv(out_stats, sep=\"\\t\")\n",
    "\n",
    "    # use freebayes call to modify the reference (reference mask to N)\n",
    "# look at reads that span more than two SNPs\n",
    "    # list(bam_fp.fetch(region=lastChr + ':' + str(block.start) + '-' + str(block.end)))\n",
    "    # see if the SNP was there or not - if it's there but the alignment is messed up we may need to modify it\n",
    "# compare haplotype calls to hg003 and hg004\n",
    "\n",
    "def reverse_compl(seq):\n",
    "    translate = {'A':'T', \n",
    "                 'T':'A', \n",
    "                 'C':'G',\n",
    "                 'G':'C'}\n",
    "    return ''.join([translate[s] for s in seq])\n",
    "\n",
    "def hamming_dist(str1, str2):\n",
    "    difference = 0\n",
    "    for x,y in zip(str1, str2):\n",
    "        if x != y:\n",
    "            difference += 1\n",
    "    return difference\n",
    "\n",
    "# get aligned portion per region\n",
    "# IPD data \n",
    "def get_matched_bases_in_read(bamread, in_pos):\n",
    "    bpos = bamread.get_aligned_pairs(matches_only=True)\n",
    "    positions = [i[0] for i in bpos]\n",
    "    refpos =  [i[1] for i in bpos] # positions in reference\n",
    "    refmap = dict(zip(refpos, positions))\n",
    "    outseq = [bamread.seq[refmap[i]] if i in refmap else 'N' for i in in_pos]\n",
    "    outseq = ''.join(outseq)\n",
    "    return outseq, ''\n",
    "\n",
    "def reverse_compl(seq):\n",
    "    translate = {'A':'T', \n",
    "                 'T':'A', \n",
    "                 'C':'G',\n",
    "                 'G':'C'}\n",
    "    return ''.join([translate[s] for s in seq])\n",
    "\n",
    "# we can make this program phase the remaining reads\n",
    "def unphased_heuristics(hair_reader, block_reader, bam_fp):\n",
    "    #1. identify unphased reads per block\n",
    "    unphased_reads = dict()\n",
    "    uphase_read_set = set()\n",
    "    #pbar = ProgressBar(len(block_reader.blocks))\n",
    "    for bid, block in block_reader.blocks.iteritems():\n",
    "        block.add_unphased_reads(bam_fp)\n",
    "        pbar2 = ProgressBar(len(block.unphased_reads))\n",
    "        for bamread in block.unphased_reads:\n",
    "            pbar2.animate()\n",
    "            bs, be = bamread.reference_start, bamread.reference_end # 0-based indexing\n",
    "            br_variants = []\n",
    "            for variant in block.variants.values():\n",
    "                if (variant.pos >= bs) & (variant.pos <= be):\n",
    "                    len_var = len(variant.r_allele)\n",
    "                    var_id = variant.var_id\n",
    "                    refcall = variant.r_allele \n",
    "                    altcall = variant.v_allele.split(',') # array of possible alternate alleles\n",
    "                    read_bases, qual_vals = get_matched_bases_in_read(bamread,range(variant.pos-1, variant.pos-1+len_var))\n",
    "                    # now let's do some primitive variant calling\n",
    "                    read_call = None # this will be an integer corresponding to the variant call of the read\n",
    "                    read_dist = [hamming_dist(refcall, read_bases)]\n",
    "                    for a in altcall:\n",
    "                        read_dist.append(hamming_dist(a, read_bases))\n",
    "                    mindist = np.min(read_dist)\n",
    "                    min_allele = [i for i, x in enumerate(read_dist) if x == mindist]\n",
    "                    if len(min_allele) == 1:\n",
    "                        read_call = min_allele[0]\n",
    "                    # if we got a variant call, let's add it to the array\n",
    "                    if read_call != None:\n",
    "                        br_variants.append([var_id, read_call, qual_vals])\n",
    "            # end variant loop\n",
    "            if len(br_variants) > 0:\n",
    "                # let's print a hairreader line\n",
    "                ranges = []\n",
    "                qline = ''\n",
    "                for k, g in groupby(enumerate(br_variants), lambda (i,x):i-x[0]):\n",
    "                    group = map(itemgetter(1), g)\n",
    "                    ranges.append((group[0][0], group))\n",
    "                hairline = [str(len(ranges)), bamread.query_name]\n",
    "                for gid, group in ranges:\n",
    "                    hairline.append(str(gid))\n",
    "                    gline = ''\n",
    "                    for g in group:\n",
    "                        gline += str(g[1])\n",
    "                        qline += g[2]\n",
    "                    hairline.append(gline)\n",
    "                hairline.append(qline)\n",
    "                hairline = ' '.join(hairline)\n",
    "                '''this now exactly matches the hairs file format'''\n",
    "                # print hairline \n",
    "                if bamread.query_name not in uphase_read_set:\n",
    "                    hairread = HapCutRead(hairline)\n",
    "                    hairread.chrom = block.chrom\n",
    "                    hairread.start = bs\n",
    "                    hairread.end = be\n",
    "                    hairread.blockcount = 1\n",
    "                    hairread.blocks = [[(pos,str(allele)) for pos, allele, qual in br_variants]]\n",
    "                    unphased_reads[bamread.query_name] = hairread\n",
    "                    uphase_read_set.add(bamread.query_name)\n",
    "                else:\n",
    "                    other_read = unphased_reads[bamread.query_name]\n",
    "                    other_read.blockcount += 1\n",
    "                    other_read.blocks.append([(pos,str(allele)) for pos, allele, qual in br_variants])\n",
    "                    other_read.positions.extend([pos for pos, allele, qual in br_variants])\n",
    "                    other_read.alleles.extend([str(allele) for pos, allele, qual in br_variants])\n",
    "            # end check for variants in bamread\n",
    "            # break\n",
    "        # end bamread loop\n",
    "        # break\n",
    "    # end block loop\n",
    "    return unphased_reads\n",
    "\n",
    "# run the program if called from the command line\n",
    "#if __name__ == \"__main__\":\n",
    "#   main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/'\n",
    "bam_fp = pysam.Samfile(basedir + 'hg002_000000F.new.merged.bam', 'rb')\n",
    "out_fp = pysam.AlignmentFile(basedir +'hg002.000000F.het_sites_truth_partition_snps_only.bam', 'wb', template=bam_fp)\n",
    "\n",
    "def truth_partition(bam_fp, het_sites, out_fp):\n",
    "    pbar = ProgressBar(450000)\n",
    "    br_variants = []\n",
    "    reads_written = set()\n",
    "    for k, line in het_sites.iterrows():\n",
    "        chrom, pos, ref, alt, hg002_genotype = line\n",
    "        pos = int(pos)\n",
    "        for read in bam_fp.fetch(reference=chrom, start=pos, end=pos+1):\n",
    "            read_vars = []\n",
    "            read_hap = 0\n",
    "            if read.qname in reads_written:\n",
    "                continue\n",
    "            else:\n",
    "                reads_written.add(read.qname)\n",
    "            pbar.animate()\n",
    "            readstart, readend = read.reference_start, read.reference_end\n",
    "            # determine all sites read overlaps\n",
    "            read_sites = het_sites[(het_sites['pos'] >= readstart) & (het_sites['pos'] <= readend)]\n",
    "            for ix, site in read_sites.iterrows():\n",
    "                vals = [len(i) for i in site[3]]\n",
    "                vals.append(len(site[2]))\n",
    "                len_var = np.min(vals)\n",
    "                sitepos = site[1]\n",
    "                refcall = site[2] \n",
    "                altcall = site[3] # array of possible alternate alleles\n",
    "                read_bases, qual_vals = get_matched_bases_in_read(read,range(sitepos-1, sitepos-1+len_var))\n",
    "                # now let's do some primitive variant calling\n",
    "                read_call = None # this will be an integer corresponding to the variant call of the read\n",
    "                read_dist = [hamming_dist(refcall, read_bases)]\n",
    "                for a in altcall:\n",
    "                    read_dist.append(hamming_dist(a, read_bases))\n",
    "                mindist = np.min(read_dist)\n",
    "                min_allele = [i for i, x in enumerate(read_dist) if x == mindist]\n",
    "                if len(min_allele) == 1:\n",
    "                    read_call = min_allele[0]\n",
    "                # if we got a variant call, let's add it to the array\n",
    "                if read_call != None:\n",
    "                    hap = None\n",
    "                    mat, pat = [int(a) for a in hg002_genotype.split(\"|\")]\n",
    "                    if read_call == mat:\n",
    "                        hap = -1\n",
    "                    if read_call == pat:\n",
    "                        hap = 1\n",
    "                    br_variants.append([chrom, pos, ref, alt, read_bases, read_call, hap])\n",
    "                    read_vars.append(hap)\n",
    "            if len(read_vars) > 0:\n",
    "                if sum(read_vars) < 0:\n",
    "                    read_hap = 1\n",
    "                elif sum(read_vars) > 0:\n",
    "                    read_hap = 2\n",
    "                else:\n",
    "                    read_hap = 0\n",
    "                haptag = [(\"ZH\", read_hap), (\"ZB\", 'truth'), (\"ZV\", len(read_vars)), ('ZS', abs(sum(read_vars)))]\n",
    "                read.tags += haptag\n",
    "                out_fp.write(read)\n",
    "    return br_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe(block):\n",
    "    for i in block.__dict__.keys():\n",
    "        item = getattr(block, i)\n",
    "        ctype = type(getattr(block, i))\n",
    "        print i, ctype\n",
    "        if (ctype == type(0)) | (ctype == type(np.int64(0))):\n",
    "            print '\\t', item\n",
    "        elif ctype == type({}):\n",
    "            print '\\t', item.keys()[0:1], '...'\n",
    "        elif ctype == type([]):\n",
    "            print '\\t', item[0:1], '...'\n",
    "        elif (ctype == type(set())) | (ctype == type(frozenset())):\n",
    "            print '\\t', list(item)[0:1], '...'\n",
    "\n",
    "def updateTranslate(block_reader):\n",
    "    translate = block_reader.translate\n",
    "    for bid, block in block_reader.blocks.iteritems():\n",
    "        for v in block.variant_ids:\n",
    "            translate[v] = block.offset\n",
    "    return translate\n",
    "\n",
    "def addBlocks(block, block2, b1hap, b2hap):\n",
    "    '''\n",
    "    self.offset      = int(ll[2])\n",
    "    self.total_len   = int(ll[4])\n",
    "    self.phased      = int(ll[6])\n",
    "    self.span        = int(ll[8])\n",
    "    self.MECscore    = float(ll[10])\n",
    "    self.fragments   = int(ll[12])\n",
    "    self.variants \t = dict() # default to empty\n",
    "    self.variant_ids = set()\n",
    "    self.chrom = 0\n",
    "    self.start = 0\n",
    "    self.end = 0\n",
    "    self.informative_reads = []\n",
    "    self.unphased_reads = []\n",
    "    self.unphased_read_set = set()\n",
    "    self.read_count = 0\n",
    "    self.read_set = set()\n",
    "    '''\n",
    "    if block.offset == block2.offset:\n",
    "        sys.stderr.write(\"Warning: blocks already merged.\")\n",
    "        sys.stderr.flush()\n",
    "        return 1\n",
    "    if block.chrom != block2.chrom:\n",
    "        raise AwesomeError(\"Can't add blocks on different contigs.\")\n",
    "    block.offset = block.offset if block2.offset > block.offset else block2.offset\n",
    "    block.MECscore += block2.MECscore\n",
    "    block.end = block2.end if block2.end > block.end else block.end\n",
    "    block.start = block2.start if block2.start < block.start else block.start\n",
    "    block.fragments += block2.fragments\n",
    "    block.informative_reads.extend(block2.informative_reads)\n",
    "    block.read_count += block2.read_count\n",
    "    block.total_len += block2.total_len\n",
    "    block.span = block.end - block.start\n",
    "    block.variant_ids = block.variant_ids.union(block2.variant_ids)\n",
    "    block.read_set = block.read_set.union(block2.read_set)\n",
    "    block.unphased_reads.extend(block2.unphased_reads)\n",
    "    block.phased += block2.phased\n",
    "    block.unphased_read_set = block.unphased_read_set.union(block2.unphased_read_set)\n",
    "    \n",
    "    for v in block2.variants:\n",
    "        haps = v.hap1, v.hap2\n",
    "        v.hap1 = haps[1] if b1hap != b2hap else haps[0]\n",
    "        v.hap2 = haps[0] if b1hap != b2hap else haps[1]\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basedir = '/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/'\n",
    "bam_fp = pysam.Samfile(basedir + 'hg002_000000F.new.merged.bam', 'rb')\n",
    "out_fp = pysam.AlignmentFile(basedir +'ipds_fwd/test_sig_denovo.bam', 'wb', template=bam_fp)\n",
    "hairs_file = basedir + 'ipds_fwd/test_sig_denovo'\n",
    "hapcut_file = basedir + 'ipds_fwd/test_hapcut_sig_denovo.hapcut'\n",
    "\n",
    "hair_reader = HairReader(hairs_file)\n",
    "block_reader = HapCutReader(hapcut_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hair_reader.assemble_blocks(block_reader)\n",
    "unphased_reads = unphased_heuristics(hair_reader, block_reader, bam_fp)\n",
    "stats_file = basedir + 'hapcut_qv13_mq10/hg002.000000F.qv13mq10.redo.merged.iblockstats.tsv'\n",
    "tag_reads(bam_fp, hair_reader, block_reader, unphased_reads, out_fp) #begin tagging reads\n",
    "interblock_stats(hair_reader, block_reader, stats_file, bam_fp) #generate interblock stats\n",
    "out_fp.close() #IMPORTANT\n",
    "\n",
    "import pickle\n",
    "pickle.dump(unphased_reads, open('hg002_more_hapcut_unphased_reads.json', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[**************   37%                  ]  166898 of 450000 complete Elapsed: 202.2 minutes\t Remaining: 343.0 minutes\n"
     ]
    }
   ],
   "source": [
    "variants = truth_partition(bam_fp, het_sites, out_fp)\n",
    "out_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[******           17%                  ]  22157 of 130000 complete Elapsed: 0.0 minutes\t Remaining: 0.1 minutes\n"
     ]
    }
   ],
   "source": [
    "vcf_truth = ht.VCF_Reader(\"/hpc/users/neffr01/jason_new/trio_analysis/aj_trio_phasebytrans.phased-trio.snps.000000F.vcf\")\n",
    "vcf_truth.parse_meta()\n",
    "vcf_truth.make_info_dict()\n",
    "het_sites = [] # an array of het sites that we will read into mem\n",
    "pbar = ProgressBar(130000)\n",
    "for var in vcf_truth:\n",
    "    pbar.animate()\n",
    "    hg002_genotype = var.samples['hg002']['GT']\n",
    "    if var.chrom != \"000000F\":\n",
    "        print \"Done\"\n",
    "        break\n",
    "    if '|' in hg002_genotype:\n",
    "        if len(set(hg002_genotype.split(\"|\"))) == 2:\n",
    "            het_sites.append([var.chrom, var.pos.pos, var.ref, var.alt, hg002_genotype])\n",
    "het_sites = pd.DataFrame(het_sites, columns=[\"chrom\", \"pos\", \"ref\", \"alt\", \"genotype\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71313"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unphased_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_phaseable2 = block_phaseable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unphasedreads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-dd0b72dc03a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddReadsToBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munphased_reads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhair_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mallphaseable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munphased_read_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munphasedreads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallphaseable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munphased_reads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munphased_reads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munphased_reads\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munphased_read_set\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mbid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhairreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhairreads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munphased_read_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unphasedreads' is not defined"
     ]
    }
   ],
   "source": [
    "block_phaseable = []\n",
    "block_index = sorted(block_reader.blocks.keys())\n",
    "block = block_reader.loc(block_index[0])\n",
    "block.addReadsToBlock(dict(unphased_reads, **hair_reader.reads))\n",
    "\n",
    "for ix, bid in enumerate(block_index[1:len(block_index)]):\n",
    "    block = block_reader.loc(bid)\n",
    "    block.addReadsToBlock(hair_reader.reads)\n",
    "    hairreads = block.read_set\n",
    "    block.addReadsToBlock(dict(unphased_reads, **hair_reader.reads))\n",
    "    block.add_unphased_reads(bam_fp)\n",
    "    allphaseable = block.read_set\n",
    "    block.unphased_read_set = unphasedreads.difference(allphaseable)\n",
    "    block.unphased_reads = {x: block.unphased_reads[x] for x in block.unphased_reads if x in block.unphased_read_set}\n",
    "    print bid, len(hairreads), len(block.read_set.difference(hairreads)), len(block.read_set), len(block.unphased_read_set)\n",
    "    print len(block.interblock_reads(block_reader.loc(block_index[ix-1]).informative_reads))\n",
    "    block_phaseable.append([bid, len(hairreads), \n",
    "                            len(block.read_set.difference(hairreads)), \n",
    "                            len(block.read_set), len(block.unphased_read_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unphased_read_set = frozenset(unphased_reads.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 2609 4936 4143 9079\n",
      "1 1990 15 72 116 188\n",
      "2 2036 88 341 597 938\n",
      "3 2282 100 270 413 683\n",
      "4 2639 78 202 396 598\n",
      "5 2823 829 2380 1499 3879\n",
      "6 4032 3856 7044 1727 8771\n",
      "7 7395 249 816 585 1401\n",
      "8 7980 3 59 73 132\n",
      "9 8031 41 198 210 408\n",
      "10 8171 3085 5944 1078 7022\n",
      "11 10922 922 1780 548 2328\n",
      "12 11744 9 58 149 207\n",
      "13 11799 13 62 268 330\n",
      "14 11852 78 209 236 445\n",
      "15 11991 145 421 663 1084\n",
      "16 12493 2665 5069 1447 6516\n",
      "17 15379 4761 8725 2211 10936\n",
      "18 19238 2207 3941 2478 6419\n",
      "19 21841 1 68 64 132\n",
      "20 21867 162 559 461 1020\n",
      "21 22338 2295 4521 1640 6161\n",
      "22 24817 3856 6606 1498 8104\n",
      "23 28033 1304 2147 1510 3657\n",
      "24 29447 144 542 583 1125\n",
      "25 29941 1 55 110 165\n",
      "26 30019 14 60 62 122\n",
      "27 30052 65 388 270 658\n",
      "28 30341 27 125 284 409\n",
      "29 30414 6 66 84 150\n",
      "30 30444 4 4 83 87\n",
      "31 30471 2543 4291 1232 5523\n",
      "32 32711 47 265 229 494\n",
      "33 32834 2375 4146 1774 5920\n",
      "34 35631 101 299 177 476\n",
      "35 35855 62 184 43 227\n",
      "36 36028 358 1012 985 1997\n",
      "37 36929 12 63 31 94\n",
      "38 36958 50 51 164 215\n",
      "39 37086 159 598 377 975\n",
      "40 37528 1302 2480 645 3125\n",
      "41 39159 263 695 767 1462\n",
      "42 39613 56 62 255 317\n",
      "43 39761 2 54 15 69\n",
      "44 39788 1444 2828 1427 4255\n",
      "45 41338 5497 9541 2045 11586\n",
      "46 46202 903 1492 174 1666\n",
      "47 47064 999 1672 433 2105\n",
      "48 47949 1476 2849 655 3504\n",
      "49 49385 22 137 173 310\n",
      "50 49475 2999 5043 1399 6442\n",
      "51 51605 17 55 208 263\n",
      "52 51669 18 92 20 112\n",
      "53 51716 16 79 92 171\n",
      "54 51752 21 79 96 175\n",
      "55 51787 17 75 22 97\n",
      "56 51836 957 1480 185 1665\n",
      "57 52611 113 383 211 594\n",
      "58 52726 89 129 773 902\n",
      "59 52883 10 127 42 169\n",
      "60 52913 4806 7605 1020 8625\n",
      "61 56322 1670 2828 616 3444\n",
      "62 57573 3445 6361 1955 8316\n",
      "63 60864 1752 3429 893 4322\n",
      "64 62295 100 221 199 420\n",
      "65 62493 35 150 210 360\n",
      "66 62596 128 408 330 738\n",
      "67 62797 409 640 172 812\n",
      "68 63220 1143 2398 600 2998\n",
      "69 64057 31 143 29 172\n",
      "70 64081 60 184 197 381\n",
      "71 64173 365 843 277 1120\n",
      "72 64483 551 1041 512 1553\n",
      "73 64997 2718 5285 1091 6376\n",
      "74 67349 94 260 174 434\n",
      "75 67550 20 20 133 153\n",
      "76 67618 245 687 854 1541\n",
      "77 68068 47 187 201 388\n",
      "78 68207 2 68 108 176\n",
      "79 68260 20 184 167 351\n",
      "80 68369 519 990 549 1539\n",
      "81 68909 85 343 471 814\n",
      "82 69151 4 4 148 152\n",
      "83 69173 21 115 10 125\n",
      "84 69211 2190 3732 746 4478\n",
      "85 71188 70 188 97 285\n",
      "86 71287 138 363 321 684\n",
      "87 71573 2344 4477 1331 5808\n",
      "88 73954 461 1044 487 1531\n",
      "89 74475 10 83 66 149\n",
      "90 74529 6329 10335 2149 12484\n",
      "91 79985 2195 3156 454 3610\n",
      "92 81800 110 304 237 541\n",
      "93 82023 1340 2447 557 3004\n",
      "94 83359 37 159 242 401\n",
      "95 83482 162 313 186 499\n",
      "96 83617 177 594 815 1409\n",
      "97 83969 18 103 57 160\n",
      "98 84020 25 172 250 422\n",
      "99 84176 18 18 248 266\n",
      "100 84236 29 178 138 316\n",
      "101 84274 288 538 465 1003\n",
      "102 84648 127 526 287 813\n",
      "103 84906 72 95 266 361\n",
      "104 85027 137 345 411 756\n",
      "105 85332 8845 15078 3168 18246\n",
      "106 91972 336 605 621 1226\n",
      "107 92451 4 59 20 79\n",
      "108 92490 159 578 579 1157\n",
      "109 92899 3 3 110 113\n",
      "110 92920 16 16 160 176\n",
      "111 93007 13 60 198 258\n",
      "112 93091 96 604 424 1028\n",
      "113 93363 3 80 25 105\n",
      "114 93405 265 856 759 1615\n",
      "115 93919 503 1438 666 2104\n",
      "116 94812 1933 3536 757 4293\n",
      "117 96587 1531 2778 1477 4255\n",
      "118 98167 1912 4036 1530 5566\n",
      "119 100045 161 541 540 1081\n",
      "120 100291 42 104 248 352\n",
      "121 100425 64 177 92 269\n",
      "122 100499 824 1783 540 2323\n",
      "123 101228 9 64 77 141\n",
      "124 101285 83 211 129 340\n",
      "125 101494 86 215 232 447\n",
      "126 101621 682 1486 1227 2713\n",
      "127 102510 1208 2071 852 2923\n",
      "128 103520 3433 5760 1613 7373\n",
      "129 105874 9 9 182 191\n",
      "130 105933 23 86 141 227\n",
      "131 106054 15 120 238 358\n",
      "132 106132 2 58 71 129\n",
      "133 106155 71 227 325 552\n",
      "134 106269 80 164 74 238\n",
      "135 106336 60 229 182 411\n",
      "136 106495 67 272 550 822\n",
      "137 106700 48 180 108 288\n",
      "138 106818 10 107 11 118\n",
      "139 106871 120 433 609 1042\n",
      "140 107264 4 76 36 112\n",
      "141 107352 12 12 64 76\n",
      "142 107381 73 301 454 755\n",
      "143 107567 1 1 150 151\n",
      "144 107664 54 259 395 654\n",
      "145 107802 3610 5917 758 6675\n",
      "146 110762 24 57 42 99\n",
      "147 110816 752 2091 1226 3317\n",
      "148 111719 30 99 75 174\n",
      "149 111783 3055 5107 975 6082\n",
      "150 114248 18 64 53 117\n",
      "151 114279 46 194 181 375\n",
      "152 114503 49 192 136 328\n",
      "153 114595 655 1012 263 1275\n",
      "154 115198 1509 2300 459 2759\n",
      "155 116446 30 269 244 513\n",
      "156 116556 2824 4374 747 5121\n",
      "157 118718 19 178 127 305\n",
      "158 118842 32 240 154 394\n",
      "159 119025 589 980 444 1424\n",
      "160 119511 44 185 349 534\n",
      "161 119609 241 971 553 1524\n",
      "162 120115 16 72 87 159\n",
      "163 120141 75 397 443 840\n",
      "164 120402 23 86 164 250\n",
      "165 120476 2034 3219 402 3621\n",
      "166 122092 80 372 132 504\n",
      "167 122203 4747 8109 1267 9376\n",
      "168 126401 4459 8397 1660 10057\n",
      "169 130462 39 141 164 305\n",
      "170 130610 2 67 39 106\n",
      "171 130668 21 172 91 263\n",
      "172 130748 98 242 282 524\n",
      "173 130916 3 91 29 120\n",
      "174 130969 62 231 155 386\n",
      "175 131089 1187 2079 367 2446\n",
      "176 132034 546 1172 419 1591\n",
      "177 132532 3121 4909 825 5734\n",
      "178 134793 217 425 52 477\n",
      "179 134934 108 201 117 318\n",
      "180 135073 4223 7310 1417 8727\n",
      "181 138444 2978 5174 1729 6903\n"
     ]
    }
   ],
   "source": [
    "for ix, bid in enumerate(block_index):\n",
    "    block = block_reader.loc(bid)\n",
    "    if len(block.unphased_read_set.intersection(block.read_set)) == 0:\n",
    "        block.addReadsToBlock(dict(unphased_reads, **hair_reader.reads))\n",
    "    print ix, bid, len(block.read_set.difference(unphased_read_set)), len(block.read_set), len(block.unphased_read_set.difference(block.read_set)), len(block.read_set.union(block.unphased_read_set))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(block_phaseable).to_csv('/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_qv1_mq1/block_phaseable.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "[(4032, 1), (2823, 1), 1.0, 1, 744157, 310402, 9927, 1091023, 1100950]\n",
      "[(7980, 1), (8031, 1), 1.0, 11, 5930, 29777, 4110, 1982896, 1987006]\n",
      "[(11744, 1), (10922, 2), 1.0, 1, 12425, 185792, 15903, 2787419, 2803322]\n",
      "[(11852, 2), (11799, 1), 1.0, 1, 25303, 19767, 7702, 2847178, 2854880]\n",
      "[(15379, 1), (19238, 1), 1.0, 2, 938799, 575875, 12030, 4495257, 4507287]\n",
      "[(21841, 2), (19238, 2), 0.5714285714285714, 7, 3861, 575875, 9919, 5083162, 5093081]\n",
      "[(28033, 1), (29447, 2), 1.0, 2, 332197, 95699, 6130, 6780299, 6786429]\n",
      "[(28033, 2), (29447, 2), 1.0, 9, 332197, 95699, 6130, 6780299, 6786429]\n",
      "[(29941, 1), (29447, 1), 1.0, 1, 8851, 95699, 6966, 6882128, 6889094]\n",
      "[(29941, 2), (29447, 1), 1.0, 2, 8851, 95699, 6966, 6882128, 6889094]\n",
      "[(32834, 0), (35631, 2), 1.0, 4, 580773, 32198, 4707, 8202797, 8207504]\n",
      "[(32834, 0), (35631, 2), 1.0, 1, 580773, 32198, 4707, 8202797, 8207504]\n",
      "[(32834, 1), (35631, 1), 1.0, 2, 580773, 32198, 4707, 8202797, 8207504]\n",
      "[(37528, 0), (37086, 2), 1.0, 1, 265526, 78357, 5587, 8597339, 8602926]\n",
      "[(39761, 1), (39788, 2), 1.0, 3, 1414, 357390, 7727, 9019236, 9026963]\n",
      "[(39761, 2), (39788, 2), 1.0, 5, 1414, 357390, 7727, 9019236, 9026963]\n",
      "[(47064, 1), (46202, 2), 1.0, 1, 185982, 147781, 12200, 10519403, 10531603]\n",
      "[(47064, 2), (47949, 1), 1.0, 8, 185982, 263713, 11585, 10717585, 10729170]\n",
      "[(49385, 0), (47949, 2), 1.0, 1, 24620, 263713, 15677, 10992883, 11008560]\n",
      "[(52611, 0), (51836, 1), 1.0, 3, 40981, 139139, 7720, 11896804, 11904524]\n",
      "[(52611, 2), (51836, 1), 1.0, 1, 40981, 139139, 7720, 11896804, 11904524]\n",
      "[(52913, 1), (52883, 1), 1.0, 1, 703399, 7845, 15381, 12053422, 12068803]\n",
      "[(60864, 1), (57573, 1), 1.0, 2, 362880, 673197, 10992, 13760010, 13771002]\n",
      "[(62596, 1), (62797, 2), 1.0, 1, 53382, 63771, 11225, 14318329, 14329554]\n",
      "[(63220, 1), (62797, 2), 1.0, 1, 266219, 63771, 9353, 14393325, 14402678]\n",
      "[(64057, 0), (64081, 2), 1.0, 3, 7814, 28793, 8234, 14687052, 14695286]\n",
      "[(64057, 1), (64081, 2), 1.0, 9, 7814, 28793, 8234, 14687052, 14695286]\n",
      "[(64081, 1), (64173, 2), 1.0, 3, 28793, 85717, 8359, 14724079, 14732438]\n",
      "[(64483, 1), (64173, 2), 1.0, 4, 128704, 85717, 8518, 14818155, 14826673]\n",
      "[(67618, 1), (68068, 2), 0.8333333333333334, 12, 132672, 33184, 7148, 15729189, 15736337]\n",
      "[(67618, 2), (68068, 2), 1.0, 1, 132672, 33184, 7148, 15729189, 15736337]\n",
      "[(68369, 0), (68909, 1), 1.0, 1, 127274, 60703, 10828, 15964858, 15975686]\n",
      "[(73954, 0), (74475, 2), 1.0, 1, 137139, 7074, 11363, 17196788, 17208151]\n",
      "[(73954, 1), (74475, 2), 1.0, 2, 137139, 7074, 11363, 17196788, 17208151]\n",
      "[(94812, 0), (93919, 2), 1.0, 1, 376104, 196474, 10798, 21873451, 21884249]\n",
      "[(94812, 1), (93919, 2), 1.0, 1, 376104, 196474, 10798, 21873451, 21884249]\n",
      "[(100499, 0), (101228, 1), 1.0, 2, 193711, 5949, 10606, 23475300, 23485906]\n",
      "[(105933, 1), (106054, 2), 1.0, 1, 11678, 22215, 10030, 24707322, 24717352]\n",
      "[(106336, 2), (106269, 1), 1.0, 12, 22985, 11222, 9022, 24846422, 24855444]\n",
      "[(107802, 1), (110762, 1), 1.0, 1, 570349, 5445, 9508, 25924035, 25933543]\n",
      "[(116556, 1), (118718, 2), 1.0, 3, 449718, 20293, 8005, 27777724, 27785729]\n",
      "[(116556, 1), (116446, 1), 1.0, 1, 449718, 11621, 8086, 27319920, 27328006]\n",
      "[(116556, 1), (118718, 1), 0.6666666666666666, 3, 449718, 20293, 8005, 27777724, 27785729]\n",
      "[(116556, 1), (116446, 2), 1.0, 2, 449718, 11621, 8086, 27319920, 27328006]\n",
      "[(116556, 2), (118718, 2), 1.0, 7, 449718, 20293, 8005, 27777724, 27785729]\n",
      "[(119609, 1), (120115, 2), 1.0, 1, 131638, 5454, 11646, 28175915, 28187561]\n",
      "[(122092, 1), (120476, 1), 1.0, 1, 36562, 311222, 12704, 28630203, 28642907]\n",
      "[(122203, 0), (122092, 1), 1.0, 1, 830122, 36562, 3472, 28679469, 28682941]\n",
      "[(126401, 0), (122203, 1), 0.6666666666666666, 6, 774777, 830122, 2380, 29513063, 29515443]\n",
      "[(126401, 1), (122203, 1), 1.0, 1, 774777, 830122, 2380, 29513063, 29515443]\n",
      "[(130610, 1), (130462, 1), 1.0, 1, 2176, 22191, 17738, 30322630, 30340368]\n",
      "[(130748, 0), (130916, 1), 1.0, 11, 37965, 3009, 7184, 30433969, 30441153]\n",
      "[(130748, 2), (130916, 1), 1.0, 1, 37965, 3009, 7184, 30433969, 30441153]\n"
     ]
    }
   ],
   "source": [
    "linked_list = []\n",
    "block_list = sorted(block_reader.blocks.keys())\n",
    "def simple_func(name1, arr, cutoff, block_list):\n",
    "    outarr = []\n",
    "    block1 = name1[0]\n",
    "    if name1[1] == 0:\n",
    "        line = None\n",
    "        outarr.append(line)\n",
    "    for block, b_arr in groupby(arr, lambda x: x[0]):\n",
    "        b_arr = list(b_arr)\n",
    "        bname = list(b_arr)[0]\n",
    "        block2 = bname[0]\n",
    "        if bname[1] == 0:\n",
    "            continue\n",
    "        if abs(block_list.index(block1) - block_list.index(block2)) > 1:\n",
    "            continue\n",
    "        b_arr = [b[1] for b in b_arr]\n",
    "        for name, group in groupby(b_arr):\n",
    "            line = []\n",
    "            group = list(group)\n",
    "            if len(group)/float(len(b_arr)) > cutoff:\n",
    "                b1, b2 = block_reader.loc(block1), block_reader.loc(block2)\n",
    "                inbd, inbd_start, inbd_end = 0,0,0\n",
    "                if block2 > block1:\n",
    "                    inbd = b2.start - b1.end\n",
    "                    inbd_start, inbd_end = b1.end, b2.start\n",
    "                else:\n",
    "                    inbd = b1.start - b2.end\n",
    "                    inbd_start, inbd_end = b2.end, b1.start\n",
    "                line = [name1, bname, len(group)/float(len(b_arr)),\n",
    "                        len(b_arr), b1.end-b1.start, b2.end-b2.start, inbd, inbd_start, inbd_end]\n",
    "            else:\n",
    "                line = None\n",
    "            outarr.append(line)\n",
    "    return outarr\n",
    "            \n",
    "interblock = []\n",
    "for k, read in unphased_reads.iteritems():\n",
    "    if read.haplotypes == dict():\n",
    "        read = greedy_partition(read, block_reader)\n",
    "    if len(read.haplotypes) > 1:\n",
    "        interblock.append([(a,b) for a,b in read.haplotypes.iteritems()])\n",
    "interblock = sorted(interblock, key=lambda x: x[0])\n",
    "for name, group in groupby(interblock, lambda x: x[0]):\n",
    "    outarr = simple_func(name, [b[1] for b in list(group)], .5, block_list)\n",
    "    for o in outarr:\n",
    "        if o != None:\n",
    "            linked_list.append(o)\n",
    "print len(linked_list)\n",
    "for l in linked_list:\n",
    "    print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linker_dict = dict()\n",
    "for row in linked_list:\n",
    "    b1, b2 = row[0], row[1]\n",
    "    support = row[2] * row[3] \n",
    "    if b1 > b2:\n",
    "        tmp = b1\n",
    "        b1 = b2\n",
    "        b2 = tmp\n",
    "    if (b1[1] == 0) | (b2[1] == 0):\n",
    "        continue\n",
    "    if linker_dict.has_key(b1[0]):\n",
    "        [bb1, bb2, bsup] = linker_dict[b1[0]]\n",
    "        if bsup < support:\n",
    "            linker_dict[b1[0]] = [b1, b2, support]\n",
    "    else:\n",
    "        linker_dict[b1[0]] = [b1, b2, support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_reader.translate = updateTranslate(block_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, row in linker_dict.iteritems():\n",
    "    block = block_reader.loc(row[1][0])\n",
    "    b1hap = row[0][1]\n",
    "    b2hap = row[1][1]\n",
    "    offset = row[1][0]\n",
    "    for var_id, v in block.variants.iteritems():\n",
    "        if var_id < offset:\n",
    "            continue\n",
    "        haps = v.hap1, v.hap2\n",
    "        v.hap1 = haps[1] if b1hap != b2hap else haps[0]\n",
    "        v.hap2 = haps[0] if b1hap != b2hap else haps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 28033 28033 9.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 62596 62596 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 2823 2823 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 15379 15379 2.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 11799 11799 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 107802 107802 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 120476 120476 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 106269 106269 12.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 130462 130462 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 67618 67618 10.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: blocks already merged."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to merge: 15379 21841 4.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 10922 11744 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 28033 29941 2.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 7980 8031 11.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 64173 64483 4.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 64057 64081 9.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 130748 130916 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 32834 35631 2.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 116556 118718 7.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 62596 63220 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 105933 106054 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 39761 39788 5.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 119609 120115 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 47064 47949 8.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 122203 126401 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 116446 116556 2.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 93919 94812 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 73954 74475 2.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 57573 60864 2.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 64057 64173 3.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 52883 52913 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 46202 47064 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n",
      "Preparing to merge: 51836 52611 1.0\n",
      "Are you sure? This cannot be undone. (y/n)y\n"
     ]
    }
   ],
   "source": [
    "for k,row in linker_dict.iteritems():\n",
    "    b1 = block_reader.loc(row[0][0])\n",
    "    b2 = block_reader.loc(row[1][0])\n",
    "    print \"Preparing to merge:\", b1.offset, b2.offset, row[2]\n",
    "    sys.stdout.flush\n",
    "    confirm = raw_input(\"Are you sure? This cannot be undone. (y/n)\")\n",
    "    if confirm == 'y':\n",
    "        a = addBlocks(b1, b2)\n",
    "        block_reader.translate = updateTranslate(block_reader)\n",
    "        if a == 0:\n",
    "            del block_reader.blocks[row[1][0]]\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0 percent done."
     ]
    }
   ],
   "source": [
    "merged_stats = '/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_with_indels/hg002_000000F.indels.aftermerge.interblock_stats.tsv'\n",
    "interblock_stats(hair_reader, block_reader, merged_stats, bam_fp) #generate interblock stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71313\n",
      "162094\n"
     ]
    }
   ],
   "source": [
    "print len(unphased_reads)\n",
    "print len(hair_reader.reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['font.size'] = 16\n",
    "rcParams['font.family'] = 'Bitstream Vera Sans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 13] Permission denied: '/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_with_indels/unphased_vs_inform_varcount-aftermerge.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2584ac9cd713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0munphased_read_vc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malleles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munphased_reads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minform_read_vc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malleles\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhair_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munphased_read_vc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minform_read_vc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_with_indels/unphased_vs_inform_varcount-aftermerge.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minform_read_vc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"informative reads\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munphased_read_vc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"phaseable reads\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/hpc/users/neffr01/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal, **kwds)\u001b[0m\n\u001b[0;32m   1187\u001b[0m                                      \u001b[0mescapechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m                                      decimal=decimal)\n\u001b[1;32m-> 1189\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/hpc/users/neffr01/anaconda/lib/python2.7/site-packages/pandas/core/format.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m             f = com._get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m-> 1442\u001b[1;33m                                 encoding=self.encoding)\n\u001b[0m\u001b[0;32m   1443\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/hpc/users/neffr01/anaconda/lib/python2.7/site-packages/pandas/core/common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path, mode, encoding, compression)\u001b[0m\n\u001b[0;32m   2829\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2831\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2833\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 13] Permission denied: '/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_with_indels/unphased_vs_inform_varcount-aftermerge.tsv'"
     ]
    }
   ],
   "source": [
    "unphased_read_vc = [len(read.alleles) for read in unphased_reads.values()]\n",
    "inform_read_vc = [len(read.alleles) for read in hair_reader.reads.values()]\n",
    "pd.DataFrame(zip(unphased_read_vc, inform_read_vc)).to_csv('/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_with_indels/unphased_vs_inform_varcount-aftermerge.tsv', sep='\\t')\n",
    "plt.hist(inform_read_vc, bins=range(0,20), color='b', alpha=0.5, label=\"informative reads\")\n",
    "plt.hist(unphased_read_vc, bins=range(0,20), color='r', alpha=0.5, label=\"phaseable reads\")\n",
    "plt.legend()\n",
    "plt.title('Phasing between read types')\n",
    "plt.xlabel('Variants per read')\n",
    "plt.ylabel('Reads')\n",
    "plt.savefig(\"/hpc/users/neffr01/jason_new/hapcut_outputs/hg002_re_000000F/hapcut_with_indels/varcount_per_read_withindels-aftermerge.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9918\t9921\t2\t13349960\t13363167\n",
      "2010\t2007\t2\t3021394\t3035106\n",
      "15765\t15766\t2\t21552954\t21567673\n",
      "15765\t15766\t2\t21552954\t21567673\n",
      "12333\t12328\t3\t17178259\t17196788\n"
     ]
    }
   ],
   "source": [
    "for k, block in block_reader.blocks.iteritems():\n",
    "    block.read_set = set(block.read_set)\n",
    "for k, read in unphased_reads.iteritems():\n",
    "    read.assemble_blocks(block_reader)\n",
    "    if read.blockcount > 1:\n",
    "        if read.end-read.start > 60000:\n",
    "            continue\n",
    "        for rb in read.blocks:\n",
    "            sys.stdout.write(str(rb[0][0]) + '\\t')\n",
    "        sys.stdout.write('\\t'.join([str(len(read.alleles)), str(read.start), str(read.end)]))\n",
    "        sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9010, '0'), (9012, '0')]\n",
      "<Block, offset_id: 8516>\n",
      "[(9013, '0'), (9014, '0')]\n",
      "<Block, offset_id: 8516>\n"
     ]
    }
   ],
   "source": [
    "for readblock in read.blocks:\n",
    "    print readblock\n",
    "    print block_reader.loc(readblock[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ZH', '8516,1'), ('ZB', 2), ('ZV', 4)]\n"
     ]
    }
   ],
   "source": [
    "b = block_reader.loc(1138)\n",
    "for k,read in unphased_reads.iteritems():\n",
    "    if read.blockcount > 1:\n",
    "        read = greedy_partition(read, block_reader)\n",
    "        print read.haplotype_fields()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
