{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ./ipy_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports / style (run this first always)\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.core import display\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "class AwesomeError(Exception):\n",
    "     def __init__(self, value):\n",
    "         self.value = value\n",
    "         pass\n",
    "     def __str__(self):\n",
    "         return repr(self.value)\n",
    "         pass\n",
    "\n",
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
    "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
    "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
    "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
    "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
    "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
    "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "\n",
    "\n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import Bio as bp\n",
    "from Bio.Sequencing.Applications import BwaAlignCommandline as bwa_aln\n",
    "from Bio.Sequencing.Applications import BwaSamseCommandline as bwa_samse\n",
    "from Bio.Sequencing.Applications import BwaSampeCommandline as bwa_sampe\n",
    "from Bio.Sequencing.Applications import BwaIndexCommandline as bwa_index\n",
    "from Bio.Sequencing.Applications import BwaBwaswCommandline as bwa_bwasw\n",
    "import HTSeq as ht\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# imports\n",
    "import os, sys, getopt\n",
    "import pysam\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "global count\n",
    "count = 0\n",
    "\n",
    "# init main\n",
    "def main(argv):\n",
    "    hairs_file = ''\n",
    "    hapcut_file = ''\n",
    "    bam_file = ''\n",
    "    out_file = ''\n",
    "    help = 'greedy_partitioner.py -h <input.hairs> -c <input.hapcut> -i <input.bam> -o <output.ann.bam>'\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"h:c:i:o:\",[\"hairs=\",\"hapcut=\", \"input=\", \"output=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print help\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '--help':\n",
    "            print help\n",
    "            sys.exit()\n",
    "        elif opt in (\"-h\", \"--hairs\"):\n",
    "            hairs_file = arg\n",
    "        elif opt in (\"-c\", \"--hapcut\"):\n",
    "            hapcut_file = arg\n",
    "        elif opt in (\"-i\", \"--input\"):\n",
    "            bam_file = arg\n",
    "        elif opt in (\"-o\", \"--output\"):\n",
    "            out_file = arg\n",
    "        else:\n",
    "            assert False, \"unhandled option\"\n",
    "\n",
    "    assert pysam.Samfile(bam_file, 'rb'), 'ERROR: Cannot open bam file for reading.'\n",
    "    assert open(bam_file + '.bai', 'rb'), 'ERROR: bam file is not indexed!'\n",
    "    bam_fp = pysam.Samfile(bam_file, 'rb')\n",
    "\n",
    "    if out_file==None:\n",
    "        out_file = bam_file + \".ann_haplotypes_\" + time.strftime(\"%m%d%y_%H%M%S\") + '.bam'\n",
    "\n",
    "    assert pysam.AlignmentFile(out_file, \"wb\", template=bam_fp), 'ERROR: Cannot open output file for writing.'\n",
    "    out_fp = pysam.AlignmentFile(out_file, \"wb\", template=bam_fp)\n",
    "\n",
    "    assert open(hairs_file), 'ERROR: Cannot access hairs file.'\n",
    "    assert open(hapcut_file), 'ERROR: Cannot open hapcut file.'\n",
    "\n",
    "    hair_reader = HairReader(hairs_file)\n",
    "    block_reader = HapCutReader(hapcut_file)\n",
    "    stats_file = out_file + \".interblock_stats.tsv\"\n",
    "    sys.stdout.write(\"Loaded greedy_partitoner.py, beginning execution. \\n\")\n",
    "    sys.stdout.flush()\n",
    "    hair_reader.assemble_blocks(block_reader)\n",
    "    tag_reads(bam_fp, hair_reader, block_reader, out_fp) #begin tagging reads\n",
    "    interblock_stats(hair_reader, block_reader, stats_file, bam_fp) #generate interblock stats\n",
    "    bam_fp.close()\n",
    "    out_fp.close()\n",
    "\n",
    "# end of main\n",
    "\n",
    "### CLASSES ###\n",
    "\n",
    "class BlockVariant:\n",
    "    def __init__ (self, variantline):\n",
    "        # variant_id haplotype_1 haplotype_2 chromosome position refallele variantallele genotype allele_counts:genotype_likelihoods:delta:MEC_variant\n",
    "        ll = variantline.strip().split(\"\\t\")\n",
    "        var_id, hap1, hap2, chrom, pos, r_allele, v_allele, genotype, info_str = ll\n",
    "        self.chrom, self.r_allele, self.v_allele, self.info_str = chrom, r_allele, v_allele, info_str\n",
    "        self.var_id, self.hap1, self.hap2, self.pos = int(var_id), hap1, hap2, int(pos)\n",
    "        allele_counts, genotype_likelihoods, delta, MEC_variant = info_str.split(\":\")[0:4]\n",
    "        self.ref_count, self.alt_count = map(int, allele_counts.split(\",\"))\n",
    "        gen_00, gen_01, gen_11 = map(float, genotype_likelihoods.split(\",\"))\n",
    "        self.gen_like = {\"0/0\":gen_00, \"0/1\":gen_01, \"1/1\":gen_11}\n",
    "        self.delta = float(delta)\n",
    "        self.MEC_variant = MEC_variant\n",
    "    def __repr__ (self):\n",
    "        return \"<BlockVariant, var_id: %s>\" % str(self.var_id)\n",
    "\n",
    "\n",
    "class Block: # part of block reader\n",
    "    def __init__ (self, blockline):\n",
    "        # \"BLOCK: offset:\" first_variant_block \"len:\" length_of_block \"phased\": phased_variants_block SPAN: \n",
    "        # lengthspanned MECscore score fragments #fragments\n",
    "\n",
    "        ll               = blockline.strip().split()\n",
    "        self.offset      = int(ll[2])\n",
    "        self.total_len   = int(ll[4])\n",
    "        self.phased      = int(ll[6])\n",
    "        self.span        = int(ll[8])\n",
    "        self.MECscore    = float(ll[10])\n",
    "        self.fragments   = int(ll[12])\n",
    "        self.variants \t = dict() # default to empty\n",
    "        self.variant_ids = set()\n",
    "        self.chrom = 0\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        self.informative_reads = []\n",
    "        self.unphased_reads = []\n",
    "        self.unphased_read_set = set()\n",
    "        self.read_count = 0\n",
    "        self.read_set = set()\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<Block, offset_id: %s>\" % str(self.offset)\n",
    "\n",
    "    def addVariant(self, variantline):\n",
    "        variant = BlockVariant(variantline)\n",
    "        self.variants[variant.var_id] = variant\n",
    "        self.variant_ids.add(variant.var_id)\n",
    "        self.updatePosition()\n",
    "\n",
    "    def updatePosition(self): # we need to do this because sometimes the variant isn't associated with a block\n",
    "        positions = []\n",
    "        chrom = None\n",
    "        for k,variant in self.variants.iteritems():\n",
    "            if chrom == None:\n",
    "                chrom = variant.chrom\n",
    "            positions.append(variant.pos)\n",
    "        self.chrom = chrom\n",
    "        self.start = np.min(positions)\n",
    "        self.end = np.max(positions)\n",
    "\n",
    "    def addReadsToBlock(self, read_dict):\n",
    "        self.informative_reads = []\n",
    "        for k,read in read_dict.iteritems():\n",
    "            read_ids = frozenset([var[0] for block in read.blocks for var in block])\n",
    "            if len(set(read_ids).intersection(set(self.variant_ids))) > 0:\n",
    "                self.informative_reads.append(read)\n",
    "        self.read_count = len(self.informative_reads)\n",
    "        self.read_set = frozenset([x.read_id for x in self.informative_reads])\n",
    "\n",
    "    def concordance(self, input_reads):\n",
    "        ''' this should return a dict of (#T,#F) tuples per variant\n",
    "         each element is a variant's concordance with the reads\n",
    "         using the read's haplotype information, we can establish whether the read's phasing\n",
    "         is consistent with how the variant was phased '''\n",
    "        variant_concord = dict()\n",
    "        support_reads_hap2 = 0\n",
    "        against_reads_hap2 = 0\n",
    "        support_reads_hap1 = 0\n",
    "        against_reads_hap1 = 0\n",
    "        for k,variant in self.variants.iteritems():\n",
    "            for read in input_reads:\n",
    "                if variant.var_id in read.positions:\n",
    "                    read_allele = read.alleles[read.positions.index(variant.var_id)]\n",
    "                    hapstate = read.haplotypes[self.offset]\n",
    "                    if hapstate == 2:\n",
    "                        if variant.hap2 != read_allele:\n",
    "                            against_reads_hap2 += 1\n",
    "                        else:\n",
    "                            support_reads_hap2 += 1\n",
    "                    else:\n",
    "                        if variant.hap1 != read_allele:\n",
    "                            against_reads_hap1 += 1\n",
    "                        else:\n",
    "                            support_reads_hap1 += 1\n",
    "        variant_concord[self.offset] = {\"hap1\": (support_reads_hap1, against_reads_hap1), \n",
    "                                               \"hap2\": (support_reads_hap2, against_reads_hap2)}\n",
    "        return variant_concord\n",
    "\n",
    "    def variant(self, var_id):\n",
    "        try:\n",
    "            return self.variants[var_id]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def interblock_reads(self, input_reads):\n",
    "        out_reads = []\n",
    "        for read in input_reads:\n",
    "            if read.read_id not in self.read_set:\n",
    "                if read.chrom == self.chrom:\n",
    "                    if ((read.start < self.end) & (read.end > self.end)) | \\\n",
    "                        ((read.end > self.start) & (read.start < self.start)) | \\\n",
    "                        ((read.end <= self.end) & (read.start >= self.start)):\n",
    "                        out_reads.append(read)\n",
    "        return out_reads\n",
    "    \n",
    "    def add_read(self, read_obj):\n",
    "        if read_obj.read_id not in self.read_set:\n",
    "            self.informative_reads.append(read_obj)\n",
    "            self.read_set.add(read_obj.read_id)\n",
    "    \n",
    "    def add_unphased_reads(self, bam_fp):\n",
    "        reads = bam_fp.fetch(region=self.chrom + ':' + str(self.start) + '-' + str(self.end))\n",
    "        for read in reads:\n",
    "            if read.query_name not in self.read_set:\n",
    "                self.unphased_reads.append(read)\n",
    "                self.unphased_read_set.add(read.query_name)\n",
    "        return None\n",
    "\n",
    "class HapCutReader: # hair file reader\n",
    "\n",
    "    def __init__ ( self, fn ):\n",
    "        self.fn = fn\n",
    "        self.blocks = dict()\n",
    "        self.translate = dict()\n",
    "        for block in self.read_file_to_blocks(fn):\n",
    "            self.blocks[block.offset] = block\n",
    "            for v in block.variant_ids: # v is an id\n",
    "                self.translate[v] = block.offset\n",
    "\n",
    "    def loc(self, block_id):\n",
    "        try:\n",
    "            return self.blocks[self.translate[block_id]]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def read_file_to_blocks(self, fn):\n",
    "        with open(fn) as f:\n",
    "            currBlock = None\n",
    "            for l in f:\n",
    "                if l[0] == \"B\": # starting a new block\n",
    "                    currBlock = Block(l)\n",
    "                elif l[0] == \"*\": # ending a block\n",
    "                    yield currBlock\n",
    "                else:\n",
    "                    currBlock.addVariant(l)\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<HapCutReader, filename: %s>\" % self.fn\n",
    "    \n",
    "    def assemble_reads(self, hair_reader):\n",
    "        for k,block in self.blocks.iteritems():\n",
    "            block.addReadsToBlock(hair_reader.reads)\n",
    "\n",
    "class HapCutRead: #hair file line\n",
    "\n",
    "    def __init__ (self, hairline):\n",
    "        #Column 1 is the number of consecutive set of SNPs covered by the fragment, NOT haplotype blocks.\n",
    "        #Column 2 is the fragment id. \n",
    "        #Column 3 is the offset of the first block of SNPs covered by the fragment followed by the alleles at the SNPs in this block.\n",
    "        #Column 5 is the offset of the second block of SNPs covered by the fragment followed by the alleles at the SNPs in this block.\n",
    "        #...\n",
    "        #The last column is a string with the quality values (Sanger fastq format) for all the alleles covered by the fragment (concatenated for all blocks). \n",
    "        #For example, if a read/fragment covers SNPs 2,3 and 5 with the alleles 0, 1 and 0 respectively, then the input will be:\n",
    "        #2 read_id 2 01 5 0 AAC\n",
    "        #Here AAC is the string corresponding to the quality values at the three alleles. The encoding of 0/1 is arbitrary but following the VCF format, 0 is reference and 1 is alternate. \n",
    "        hairlist = hairline.strip().split()\n",
    "        self.blockcount = 0                # this information must be determined afterwards \n",
    "        self.read_id    = hairlist[1]      # read_id\n",
    "        self.blocks     = []\t\t       # an array of tuples corresponding to blocks\n",
    "        self.positions  = []\n",
    "        self.alleles    = []\n",
    "        self.chrom      = None\n",
    "        self.start      = None\n",
    "        self.end        = None\n",
    "        self.haplotypes = dict()             # an array of {\"block_offset\":\"haplotype\"} \n",
    "                                           # after partitioning\n",
    "        for i in range(2, len(hairlist)-1, 2):\n",
    "            position = int(hairlist[i])\n",
    "            allele = hairlist[i+1]\n",
    "            block = zip(range(position, position+len(allele)), allele)\n",
    "            self.blocks.append(block)\n",
    "            self.positions.extend(range(position, position+len(allele)))\n",
    "            self.alleles.extend(allele)\n",
    "            self.qualities  = hairlist[-1]         # a matched arary of the qualities of allele calls\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<HapCutRead, read_id: %s>\" % str(self.read_id)\n",
    "\n",
    "    def haplotype_fields(self):\n",
    "        haps = \";\".join([','.join([str(key), str(self.haplotypes[key])]) for key in self.haplotypes])\n",
    "        haptag = [(\"ZH\", haps), (\"ZB\", int(self.blockcount)), (\"ZV\", len(self.positions))]\n",
    "        return haptag\n",
    "    \n",
    "    def addGenomicPositions(self, block_reader):\n",
    "        arr = []\n",
    "        chrom = None\n",
    "        for position in self.positions:\n",
    "            b = block_reader.loc(position)\n",
    "            if b == None:\n",
    "                continue\n",
    "            if chrom == None:\n",
    "                chrom = b.chrom\n",
    "            arr.append(b.variant(position).pos)\n",
    "        if len(arr) > 0:\n",
    "            self.chrom = chrom\n",
    "            self.start = np.min(arr)\n",
    "            self.end = np.max(arr)\n",
    "        else:\n",
    "            self.chrom = '*'\n",
    "            self.start = None\n",
    "            self.end = None\n",
    "    \n",
    "    def assemble_blocks(self, block_reader):\n",
    "        # it turns out that the blocks provided in a hapcut file don't actually correspond to real blocks\n",
    "        # just contiguous alleles?\n",
    "        self.blocks = []\n",
    "        lastBlock = -1\n",
    "        for ix, pos in enumerate(self.positions): # self.positions corresponds to a variant id\n",
    "            currBlock = block_reader.loc(pos) # look up block associated with variant\n",
    "            if currBlock == None: # if it's not, continue\n",
    "                continue\n",
    "            # let's also add ourselves to the block\n",
    "            currBlock.add_read(self) #if we get a block back, add the read to the block's read set\n",
    "            currBlock = currBlock.offset # set our read's block id\n",
    "            if currBlock != lastBlock:\n",
    "                self.blocks.append([])\n",
    "            self.blocks[-1].append((pos, self.alleles[ix])) # a read can be more than one haplotype block long\n",
    "            lastBlock = currBlock\n",
    "        self.blockcount = len(self.blocks) # determine number of haplotype blocks read spans\n",
    "        self.addGenomicPositions(block_reader) # determine start-end positions of read\n",
    "    \n",
    "class HairReader:\n",
    "\n",
    "    def __init__ (self, fn):\n",
    "        self.fn = fn\n",
    "        self.reads = dict()\n",
    "        with open (fn) as f:\n",
    "            for l in f:\n",
    "                newread = HapCutRead(l)\n",
    "                self.reads[newread.read_id] = newread\n",
    "        self.read_set = frozenset(self.reads.keys())\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<HairReader, filename: %s>\" % self.fn\n",
    "\n",
    "    def loc(self, read_id):\n",
    "        try:\n",
    "            return self.reads[read_id]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def assemble_blocks(self, block_reader):\n",
    "        for k,read in self.reads.iteritems():\n",
    "            read.assemble_blocks(block_reader)\n",
    "\n",
    "### FUNCTIONS ###\n",
    "\n",
    "'''\n",
    "\n",
    "tag_reads()\n",
    "\n",
    "Usage: Tags reads from a bam file corresponding to a particular haplotype, with haplotype\n",
    "definitions from HapCut, under the optional tag \"ZH\".\n",
    "\n",
    "Inputs:\n",
    "    bam_fp\n",
    "    A pysam.Samfile object pointing to the input file\n",
    "    hair_reader\n",
    "    A HairReader object pointing to the hairs file.\n",
    "    block_reader\n",
    "    A HapcutReader object pointing to the hapcut file.\n",
    "    out_fp\n",
    "    A pysam.AlignmentFile pointing to the output bam.\n",
    "Outputs:\n",
    "    (none - writes to out_fp)\n",
    "\n",
    "'''\n",
    "\n",
    "def tag_reads(bam_fp, hair_reader, block_reader, out_fp):\n",
    "    ''' tag_reads(bam_fp, hair_reader, block_reader, out_fp)'''\n",
    "    global count\n",
    "    for bamread in bam_fp.fetch():\n",
    "        count += 1\n",
    "        if (count % 100) == 0:\n",
    "            sys.stdout.write(\"\\rWritten %s lines to output.\" % str(count))\n",
    "            sys.stdout.flush()\n",
    "        if bamread.query_name in hair_reader.read_set:\n",
    "            read = hair_reader.loc(bamread.query_name)\n",
    "            read = greedy_partition(read, block_reader)\n",
    "            bamread.tags += read.haplotype_fields()      # add the haplotype information\n",
    "        else:\n",
    "            read = unphased_heuristics(block_reader, bamread)\n",
    "        out_fp.write(bamread)\n",
    "\n",
    "'''\n",
    "greedy_partition()\n",
    "Ryan Neff\n",
    "\n",
    "inputs:\n",
    "read\n",
    "    a HapCutRead object\n",
    "block_reader\n",
    "    of the type HapCutReader\n",
    "\n",
    "outputs:\n",
    "    the original read, now with haplotype information.\n",
    "\n",
    "translate hairfile alleles into blockvar IDs\n",
    "get alleles in each read spanning blockvars\n",
    "determine alleles for the two blocks from blockvar\n",
    "partition read based on locally most probable alignment\n",
    "\n",
    "'''\n",
    "\n",
    "def greedy_partition(read, block_reader):\n",
    "    for readblock in read.blocks:\n",
    "        positions = [x[0] for x in readblock]\n",
    "        alleles = [x[1] for x in readblock]\n",
    "        allele_state = []\n",
    "        offset = positions[0]\n",
    "        hap = 0\n",
    "        block = block_reader.loc(offset) #retrieve block the read is in\n",
    "        if block == None: # this happens when hapcut throws out the block the read is in \n",
    "            continue\n",
    "        offset = block.offset\n",
    "        for ix, varpos in enumerate(positions):\n",
    "            blockvar = block.variant(varpos) # retrieve variant from VCF file\n",
    "            if blockvar.hap1 == alleles[ix]:\n",
    "                allele_state.append(-1)\n",
    "            elif blockvar.hap2 == alleles[ix]:\n",
    "                allele_state.append(1)\n",
    "            else:\n",
    "                #sys.stderr.write(\"\\nWarning: read allele matched no haplotypes.\")\n",
    "                #sys.stderr.write(\"\\nHair read: %s\" % read.read_id)\n",
    "                #sys.stderr.write(\"\\nAlleles: %s\" % str(alleles[ix]))\n",
    "                #sys.stderr.write(\"\\n Hap 1: %s, Hap 2: %s\\n\" % (blockvar.hap1, blockvar.hap2))\n",
    "                #sys.stderr.flush()\n",
    "                continue\n",
    "        if len(allele_state) < 1:\n",
    "            sys.stderr.write(\"Warning: no haplotype information in read.\\n\")\n",
    "            sys.stderr.write(\"\\nHair read: %s\" % read.read_id)\n",
    "            sys.stderr.write(\"\\nAlleles: %s\\n\" % str(alleles))\n",
    "            sys.stderr.flush()\n",
    "            hap = -1\n",
    "        if sum(allele_state) < 0:\n",
    "            hap = 1\n",
    "        elif sum(allele_state) > 0:\n",
    "            hap = 2\n",
    "        else:\n",
    "            hap = 0\n",
    "        read.haplotypes[offset] = hap\n",
    "    return read\n",
    "\n",
    "'''\n",
    "interblock_stats()\n",
    "\n",
    "Usage: Creates a tab-separated values file with statistics about reads overlapping\n",
    "between nearby blocks, and finds the concordance of these interblock reads\n",
    "with haplotypes in other blocks. \n",
    "\n",
    "inputs:\n",
    "    hair_reader\n",
    "        A HairReader object\n",
    "    block_reader\n",
    "        A HapcutReader object\n",
    "    out_stats\n",
    "        A string where the .tsv should be written. Defaults\n",
    "        to the hairs filename given in the input + 'interblock_stats.tsv'\n",
    "outputs:\n",
    "    none-writes to file directly\n",
    "\n",
    "'''\n",
    "\n",
    "def interblock_stats(hair_reader, block_reader, out_stats, bam_fp):\n",
    "    blockdist = []\n",
    "    lastChr = None\n",
    "    lastPos = None\n",
    "    lastBlock = None\n",
    "    lastReads = set()\n",
    "    for k,read in hair_reader.reads.iteritems():\n",
    "        if read.haplotypes == dict():\n",
    "            read = greedy_partition(read, block_reader)\n",
    "    for ix, key in enumerate(sorted(block_reader.blocks.keys())):\n",
    "        sys.stdout.write('\\r%s percent done.' % round(ix/float(len(block_reader.blocks))*100))\n",
    "        sys.stdout.flush()\n",
    "        block = block_reader.blocks[key]\n",
    "        if block.read_set == set():\n",
    "            block.addReadsToBlock(hair_reader.reads)\n",
    "        currBlock = block.offset\n",
    "        currChr = block.chrom\n",
    "        currPos = block.start\n",
    "        if lastBlock != None:\n",
    "            if lastChr == currChr:\n",
    "                interblock_reads = block.interblock_reads(lastBlock_obj.informative_reads)\n",
    "                row=[lastBlock, currBlock, currChr, lastPos, currPos, currPos-lastPos,\n",
    "                     lastBlock_obj.end-lastBlock_obj.start, block.end-block.start,\n",
    "                     len(lastBlock_obj.variant_ids), len(block.variant_ids), \n",
    "                     len(lastBlock_obj.read_set), len(block.read_set),\n",
    "                     len(list(bam_fp.fetch(region=lastChr + ':' + str(lastBlock_obj.start) + '-' + str(lastBlock_obj.end)))),\n",
    "                     len(list(bam_fp.fetch(region=lastChr + ':' + str(block.start) + '-' + str(block.end)))),\n",
    "                     len(interblock_reads),\n",
    "                     len(list(bam_fp.fetch(region=lastChr + ':' + str(lastBlock_obj.end) + '-' + str(block.start)))),\n",
    "                     lastBlock_obj.concordance(lastBlock_obj.informative_reads), \n",
    "                     block.concordance(block.informative_reads)]\n",
    "                blockdist.append(row)\n",
    "            else:\n",
    "                continue\n",
    "        lastBlock = currBlock\n",
    "        lastBlock_obj = block\n",
    "        lastChr = currChr\n",
    "        lastPos = block.end\n",
    "    header = ['block1', 'block2', 'chrom', 'block1_end', 'block2_start', \n",
    "              'distance', 'block1_size', 'block2_size', 'block1_variants', 'block2_variants', \n",
    "              'block1_informative_reads', 'block2_informative_reads', \n",
    "              'block1_reads', 'block2_reads',\n",
    "              'informative_interblock_reads', \n",
    "              'all_interblock_reads', \n",
    "              'block1_concordance', 'block2_concordance']\n",
    "    info = pd.DataFrame(blockdist, columns=header)\n",
    "    info.to_csv(out_stats, sep=\"\\t\")\n",
    "\n",
    "# run the program if called from the command line\n",
    "#if __name__ == \"__main__\":\n",
    "#   main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use freebayes call to modify the reference (reference mask to N)\n",
    "# look at reads that span more than two SNPs\n",
    "    # list(bam_fp.fetch(region=lastChr + ':' + str(block.start) + '-' + str(block.end)))\n",
    "    # see if the SNP was there or not - if it's there but the alignment is messed up we may need to modify it\n",
    "# compare haplotype calls to hg003 and hg004\n",
    "\n",
    "def reverse_compl(seq):\n",
    "    translate = {'A':'T', \n",
    "                 'T':'A', \n",
    "                 'C':'G',\n",
    "                 'G':'C'}\n",
    "    return ''.join([translate[s] for s in seq])\n",
    "\n",
    "def get_matched_bases_in_read(bamread, positions):\n",
    "    pairdict = dict()\n",
    "    outseq = []\n",
    "    outqual = []\n",
    "    for i in bamread.get_aligned_pairs(matches_only=True):\n",
    "        pairdict[i[1]]=i[0]\n",
    "    for i in positions:\n",
    "        try:\n",
    "            outseq.append(bamread.seq[pairdict[i]])\n",
    "            outqual.append(bamread.qual[pairdict[i]])\n",
    "        except:\n",
    "            outseq.append('N')\n",
    "            outqual.append('.') #this may need to be adjusted later\n",
    "    outseq = ''.join(outseq)\n",
    "    outqual = ''.join(outqual)\n",
    "    return outseq, outqual\n",
    "\n",
    "def hamming_dist(str1, str2):\n",
    "    difference = 0\n",
    "    for x,y in zip(str1, str2):\n",
    "        if x != y:\n",
    "            difference += 1\n",
    "    return difference\n",
    "\n",
    "# we can make this program phase the remaining reads\n",
    "def unphased_heuristics(hair_reader, block_reader, bam_fp):\n",
    "    #1. identify unphased reads per block\n",
    "    unphased_reads = dict()\n",
    "    pbar = ProgressBar(len(block_reader.blocks))\n",
    "    for bid, block in block_reader.blocks.iteritems():\n",
    "        block.add_unphased_reads(bam_fp)\n",
    "        pbar.animate()\n",
    "        pbar2 = ProgressBar(len(block.unphased_reads))\n",
    "        for bamread in block.unphased_reads:\n",
    "            pbar2.animate()\n",
    "            bs, be = bamread.reference_start, bamread.reference_end # 0-based indexing\n",
    "            if bamread.is_reverse:\n",
    "                seq = reverse_compl(bamread.seq)\n",
    "            else:\n",
    "                seq = bamread.seq\n",
    "            br_variants = []\n",
    "            for variant in block.variants.values():\n",
    "                len_var = len(variant.r_allele)\n",
    "                if (variant.pos >= bs) & (variant.pos <= be):\n",
    "                    var_id = variant.var_id\n",
    "                    refcall = variant.r_allele \n",
    "                    altcall = variant.v_allele.split(',') # array of possible alternate alleles\n",
    "                    read_bases, qual_vals = get_matched_bases_in_read(bamread,range(variant.pos-1, variant.pos-1+len_var))\n",
    "                    # now let's do some primitive variant calling\n",
    "                    read_call = None # this will be an integer corresponding to the variant call of the read\n",
    "                    read_dist = [hamming_dist(refcall, read_bases)]\n",
    "                    for a in altcall:\n",
    "                        read_dist.append(hamming_dist(a, read_bases))\n",
    "                    mindist = np.min(read_dist)\n",
    "                    min_allele = [i for i, x in enumerate(read_dist) if x == mindist]\n",
    "                    if len(min_allele) == 1:\n",
    "                        read_call = min_allele[0]\n",
    "                    # if we got a variant call, let's add it to the array\n",
    "                    if read_call != None:\n",
    "                        br_variants.append([var_id, read_call, qual_vals])\n",
    "            # end variant loop\n",
    "            if len(br_variants) > 0:\n",
    "                # let's print a hairreader line\n",
    "                ranges = []\n",
    "                qline = ''\n",
    "                for k, g in groupby(enumerate(br_variants), lambda (i,x):i-x[0]):\n",
    "                    group = map(itemgetter(1), g)\n",
    "                    ranges.append((group[0][0], group))\n",
    "                hairline = [str(len(ranges)), bamread.query_name]\n",
    "                for gid, group in ranges:\n",
    "                    hairline.append(str(gid))\n",
    "                    gline = ''\n",
    "                    for g in group:\n",
    "                        gline += str(g[1])\n",
    "                        qline += g[2]\n",
    "                    hairline.append(gline)\n",
    "                hairline.append(qline)\n",
    "                hairline = ' '.join(hairline)\n",
    "                '''this now exactly matches the hairs file format'''\n",
    "                # print hairline \n",
    "                if bamread.query_name not in unphased_reads:\n",
    "                    hairread = HapCutRead(hairline)\n",
    "                    hairread.chrom = block.chrom\n",
    "                    hairread.start = bs\n",
    "                    hairread.end = be\n",
    "                    hairread.blockcount = 1\n",
    "                    hairread.blocks = [[(pos,str(allele)) for pos, allele, qual in br_variants]]\n",
    "                    unphased_reads[bamread.query_name] = hairread\n",
    "                else:\n",
    "                    other_read = unphased_reads[bamread.query_name]\n",
    "                    other_read.blockcount += 1\n",
    "                    other_read.blocks.append([(pos,str(allele)) for pos, allele, qual in br_variants])\n",
    "                    other_read.positions.extend([pos for pos, allele, qual in br_variants])\n",
    "                    other_read.alleles.extend([str(allele) for pos, allele, qual in br_variants])\n",
    "            # end check for variants in bamread\n",
    "            # break\n",
    "        # end bamread loop\n",
    "        # break\n",
    "    # end block loop\n",
    "    return unphased_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*****************98%***************** ]  12555 of 12766 complete Elapsed: 4.2 minutes\t Remaining: 0.1 minutes"
     ]
    }
   ],
   "source": [
    "unphased_reads = unphased_heuristics(hair_reader, block_reader, bam_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interblock statistics**\n",
    "\n",
    "* length of block\n",
    "* variant count in block\n",
    "* total reads inside block\n",
    "* Informative reads inside block\n",
    "* Distance between blocks\n",
    "* Which blocks overlap\n",
    "* Interblock reads\n",
    "    * Read count between junctions\n",
    "    * 2x2 matrix with support for linking blocks if at junction\n",
    "* Total coverage between blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bam_fp = pysam.Samfile('/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/hg002_000000F.new.merged.bam', 'rb')\n",
    "out_fp = pysam.AlignmentFile('/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/hg002.000000F.new.merged.ann.bam', 'wb', template=bam_fp)\n",
    "hairs_file = '/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/hg002_hapcut_000000F.hairs'\n",
    "hapcut_file = '/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/hg002_hapcut_000000F.hapcut'\n",
    "\n",
    "hair_reader = HairReader(hairs_file)\n",
    "block_reader = HapCutReader(hapcut_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5407614\n"
     ]
    }
   ],
   "source": [
    "for k, v in hair_reader.reads.iteritems():\n",
    "    print v.start\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pysam.csamfile.Samfile object at 0x7f46fbed9f50>\n"
     ]
    }
   ],
   "source": [
    "print bam_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Block, offset_id: 10273>\n",
      "block: 13825792 13981531\n",
      "684\n",
      "1840\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "v = block_reader.loc(10273)\n",
    "print v\n",
    "print \"block:\", v.start, v.end\n",
    "for read in v.informative_reads:\n",
    "    read.addGenomicPositions(block_reader)\n",
    "    if read.chrom != \"000000F\":\n",
    "        print read.chrom\n",
    "    if read.start > v.end:\n",
    "        print read.start\n",
    "    if read.end < v.start:\n",
    "        print read.end\n",
    "v.addReadsToBlock(hair_reader.reads)\n",
    "print len(v.read_set)\n",
    "bam_ids = set([i.query_name for i in list(bam_fp.fetch(region=v.chrom + ':' + str(v.start) + '-' + str(v.end)))])\n",
    "print len(bam_ids)\n",
    "print len(v.read_set.intersection(bam_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000F:2453853-2476920\n"
     ]
    }
   ],
   "source": [
    "print v.chrom + ':' + str(v.start) + '-' + str(v.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hair_reader.assemble_blocks(block_reader)\n",
    "stats_file = '/sc/orga/scratch/bashia02/collaborations/hardik_shah/jason_new/hapcut_outputs/hg002_re_000000F/hg002.000000F.interblock_stats.tsv'\n",
    "#tag_reads(bam_fp, hair_reader, block_reader, out_fp) #begin tagging reads\n",
    "#interblock_stats(hair_reader, block_reader, stats_file, bam_fp) #generate interblock stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "read = unphased_heuristics(hair_reader, block_reader, bam_fp)['m150106_014109_42177R_c100761782550000001823161607221527_s1_p0/73911/0_18036']\n",
    "read2 = hair_reader.loc(hair_reader.reads.keys()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1'] [[(10240, '1')]] [10240]\n",
      "['1', '0', '0', '0', '0', '0'] [[(23177, '1'), (23179, '0'), (23180, '0'), (23182, '0'), (23185, '0'), (23187, '0')]] [23177, 23179, 23180, 23182, 23185, 23187]\n"
     ]
    }
   ],
   "source": [
    "print read.alleles, read.blocks, read.positions\n",
    "print read2.alleles, read2.blocks, read2.positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ZH', '10240,1'), ('ZB', 1), ('ZV', 1)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read3 = greedy_partition(read, block_reader)\n",
    "read.haplotype_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Block, offset_id: 10240>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_reader.loc(10240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x,block in enumerate(block_reader.blocks):\n",
    "    lastBlock_obj = block_reader.blocks[x-1]\n",
    "    interblock_reads = block.interblock_reads(hair_reader.reads)\n",
    "    #print interblock_reads\n",
    "    print lastBlock_obj.start\n",
    "    print lastBlock_obj.end\n",
    "    print [x.start for x in block.informative_reads]\n",
    "    print interblock_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97551\n",
      "97551\n"
     ]
    }
   ],
   "source": [
    "ir = []\n",
    "count = 0\n",
    "for block in block_reader.blocks:\n",
    "    ir.extend(block.informative_reads)\n",
    "    count += len(block.informative_reads)\n",
    "print len(ir)\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 percent done."
     ]
    }
   ],
   "source": [
    "interblock_stats(hair_reader, block_reader, hairs_file + \".interblock_stats.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1305"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(bam_fp.fetch(region='000000F:36129-75950')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdict = {'hello':'hey', 'foo':'bar'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo bar\n",
      "hello hey\n"
     ]
    }
   ],
   "source": [
    "for k,v in testdict.iteritems():\n",
    "    print k, v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
