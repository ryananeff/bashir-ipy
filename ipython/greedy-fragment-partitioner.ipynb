{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minerva4\r\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "try:\n",
    "    import readline\n",
    "except ImportError:\n",
    "    print(\"Module readline not available.\")\n",
    "else:\n",
    "    import rlcompleter\n",
    "    readline.parse_and_bind(\"tab: complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports / style (run this first always)\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import FileLink, FileLinks\n",
    "from IPython.core import display\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "class AwesomeError(Exception):\n",
    "     def __init__(self, value):\n",
    "         self.value = value\n",
    "         pass\n",
    "     def __str__(self):\n",
    "         return repr(self.value)\n",
    "         pass\n",
    "\n",
    "#colorbrewer2 Dark2 qualitative color table\n",
    "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
    "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
    "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
    "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
    "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
    "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
    "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843)]\n",
    "\n",
    "rcParams['figure.figsize'] = (10, 6)\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['axes.color_cycle'] = dark2_colors\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "\n",
    "\n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecesasry plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import Bio as bp\n",
    "from Bio.Sequencing.Applications import BwaAlignCommandline as bwa_aln\n",
    "from Bio.Sequencing.Applications import BwaSamseCommandline as bwa_samse\n",
    "from Bio.Sequencing.Applications import BwaSampeCommandline as bwa_sampe\n",
    "from Bio.Sequencing.Applications import BwaIndexCommandline as bwa_index\n",
    "from Bio.Sequencing.Applications import BwaBwaswCommandline as bwa_bwasw\n",
    "import HTSeq as ht\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# imports\n",
    "import os, sys, getopt\n",
    "import pysam\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# init\n",
    "def main(argv):\n",
    "    hairs_file = ''\n",
    "    hapcut_file = ''\n",
    "    bam_file = ''\n",
    "    out_file = ''\n",
    "    help = 'greedy_partitioner.py -h <input.hairs> -c <input.hapcut> -i <input.bam> -o <output.ann.bam>'\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"h:c:i:o:\",[\"hairs=\",\"hapcut=\", \"input=\", \"output=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print help\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '--help':\n",
    "            print help\n",
    "            sys.exit()\n",
    "        elif opt in (\"-h\", \"--hairs\"):\n",
    "            hairs_file = arg\n",
    "        elif opt in (\"-c\", \"--hapcut\"):\n",
    "            hapcut_file = arg\n",
    "        elif opt in (\"-i\", \"--input\"):\n",
    "            bam_file = arg\n",
    "        elif opt in (\"-o\", \"--output\"):\n",
    "            out_file = arg\n",
    "        else:\n",
    "            assert False, \"unhandled option\"\n",
    "\n",
    "    assert pysam.Samfile(bam_file, 'rb'), 'ERROR: Cannot open bam file for reading.'\n",
    "    assert open(bam_file + '.bai', 'rb'), 'ERROR: bam file is not indexed!'\n",
    "    bam_fp = pysam.Samfile(bam_file, 'rb')\n",
    "\n",
    "    if out_file==None:\n",
    "        out_file = bam_file + \".ann_haplotypes_\" + time.strftime(\"%m%d%y_%H%M%S\") + '.bam'\n",
    "\n",
    "    assert pysam.AlignmentFile(out_file, \"wb\", template=bam_fp), 'ERROR: Cannot open output file for writing.'\n",
    "    out_fp = pysam.AlignmentFile(out_file, \"wb\", template=bam_fp)\n",
    "\n",
    "    assert open(hairs_file), 'ERROR: Cannot access hairs file.'\n",
    "    assert open(hapcut_file), 'ERROR: Cannot open hapcut file.'\n",
    "    \n",
    "    hair_reader = HairReader(hairs_file)\n",
    "    block_reader = HapCutReader(hapcut_file)\n",
    "    \n",
    "    tag_reads(bam_fp, hair_reader, block_reader, out_fp) #begin tagging reads\n",
    "    bam_fp.close()\n",
    "    out_fp.close()\n",
    "# end of main()\n",
    "\n",
    "### CLASSES ###\n",
    "\n",
    "class BlockVariant:\n",
    "    def __init__ (self, variantline):\n",
    "        # variant_id haplotype_1 haplotype_2 chromosome position refallele variantallele genotype allele_counts:genotype_likelihoods:delta:MEC_variant\n",
    "        ll = variantline.strip().split(\"\\t\")\n",
    "        var_id, hap1, hap2, chrom, pos, r_allele, v_allele, genotype, info_str = ll\n",
    "        self.chrom, self.r_allele, self.v_allele, self.info_str = chrom, r_allele, v_allele, info_str\n",
    "        self.var_id, self.hap1, self.hap2, self.pos = int(var_id), hap1, hap2, int(pos)\n",
    "        allele_counts, genotype_likelihoods, delta, MEC_variant = info_str.split(\":\")\n",
    "        self.ref_count, self.alt_count = map(int, allele_counts.split(\",\"))\n",
    "        gen_00, gen_01, gen_11 = map(float, genotype_likelihoods.split(\",\"))\n",
    "        self.gen_like = {\"0/0\":gen_00, \"0/1\":gen_01, \"1/1\":gen_11}\n",
    "        self.delta = float(delta)\n",
    "        self.MEC_variant = MEC_variant\n",
    "    def __repr__ (self):\n",
    "        return \"<BlockVariant, var_id: %s>\" % str(self.var_id)\n",
    "\n",
    "\n",
    "class Block:\n",
    "    def __init__ (self, blockline):\n",
    "        # \"BLOCK: offset:\" first_variant_block \"len:\" length_of_block \"phased\": phased_variants_block SPAN: \n",
    "        # lengthspanned MECscore score fragments #fragments\n",
    "\n",
    "        ll               = blockline.strip().split()\n",
    "        self.offset      = int(ll[2])\n",
    "        self.total_len   = int(ll[4])\n",
    "        self.phased      = int(ll[6])\n",
    "        self.span        = int(ll[8])\n",
    "        self.MECscore    = float(ll[10])\n",
    "        self.fragments   = int(ll[12])\n",
    "        self.variants \t = [] # default to empty\n",
    "        self.variant_ids = []\n",
    "        self.chrom = 0\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        self.informative_reads = []\n",
    "        self.read_count = 0\n",
    "        self.read_set = set()\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<Block, offset_id: %s>\" % str(self.offset)\n",
    "\n",
    "    def addVariant(self, variantline):\n",
    "        variant = BlockVariant(variantline)\n",
    "        self.variants.append(variant)\n",
    "        self.variant_ids.append(variant.var_id)\n",
    "        self.updatePosition()\n",
    "    \n",
    "    def updatePosition(self):\n",
    "        positions = []\n",
    "        chrom = None\n",
    "        for variant in self.variants:\n",
    "            if chrom == None:\n",
    "                chrom = variant.chrom\n",
    "            positions.append(variant.pos)\n",
    "        self.chrom = chrom\n",
    "        self.start = np.min(positions)\n",
    "        self.end = np.max(positions)\n",
    "    \n",
    "    def addReadsToBlock(self, read_array):\n",
    "        for read in read_array:\n",
    "            read_ids = [var[0] for block in read.blocks for var in block]\n",
    "            if len(set(read_ids).intersection(set(self.variant_ids))) > 0:\n",
    "                self.informative_reads.append(read)\n",
    "        self.read_count = len(self.informative_reads)\n",
    "        self.read_set = frozenset([x.read_id for x in self.informative_reads])\n",
    "    \n",
    "    def concordance(self, input_reads):\n",
    "        #TODO\n",
    "        # this should return a dict of (#T,#F) tuples per variant\n",
    "        # each element is a variant's concordance with the reads\n",
    "        # using the read's haplotype information, we can establish whether the read's phasing\n",
    "        # is consistent with how the variant was phased\n",
    "        variant_concord = dict()\n",
    "        for variant in self.variants:\n",
    "            support_reads_hap2 = 0\n",
    "            against_reads_hap2 = 0\n",
    "            support_reads_hap1 = 0\n",
    "            against_reads_hap1 = 0\n",
    "            for read in input_reads:\n",
    "                if variant.var_id in read.positions:\n",
    "                    read_allele = read.alleles[read.positions.index(variant.var_id)]\n",
    "                    hapstate = read.haplotypes[self.offset]\n",
    "                    if hapstate == 2:\n",
    "                        if variant.hap2 != read_allele:\n",
    "                            against_reads_hap2 += 1\n",
    "                        else:\n",
    "                            support_reads_hap2 += 1\n",
    "                    else:\n",
    "                        if variant.hap1 != read_allele:\n",
    "                            against_reads_hap1 += 1\n",
    "                        else:\n",
    "                            support_reads_hap1 += 1\n",
    "            variant_concord[variant.var_id] = {\"hap1\": (support_reads_hap1, against_reads_hap1), \n",
    "                                               \"hap2\": (support_reads_hap2, against_reads_hap2)}\n",
    "        return variant_concord\n",
    "    \n",
    "    def variant(self, var_id):\n",
    "        return next((x for x in self.variants if var_id == x.var_id), None)\n",
    "    \n",
    "    def interblock_reads(self, input_block):\n",
    "        return self.read_set.intersection(input_block.read_set)\n",
    "    \n",
    "class HapCutReader:\n",
    "\n",
    "    def __init__ ( self, fn ):\n",
    "        self.fn = fn\n",
    "        self.blocks = list(self.read_file_to_blocks(fn))\n",
    "    \n",
    "    def loc(self, block_id):\n",
    "        return next((x for x in self.blocks if block_id in x.variant_ids), None)\n",
    "\n",
    "    def read_file_to_blocks(self, fn):\n",
    "        with open(fn) as f:\n",
    "            currBlock = None\n",
    "            for l in f:\n",
    "                if l[0] == \"B\": # starting a new block\n",
    "                    currBlock = Block(l)\n",
    "                elif l[0] == \"*\": # ending a block\n",
    "                    yield currBlock\n",
    "                else:\n",
    "                    currBlock.addVariant(l)\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<HapCutReader, filename: %s>\" % self.fn                    \n",
    "\n",
    "class HapCutRead:\n",
    "\n",
    "    def __init__ (self, hairline):\n",
    "        #Column 1 is the number of blocks (consecutive set of SNPs covered by the fragment). \n",
    "        #Column 2 is the fragment id. \n",
    "        #Column 3 is the offset of the first block of SNPs covered by the fragment followed by the alleles at the SNPs in this block.\n",
    "        #Column 5 is the offset of the second block of SNPs covered by the fragment followed by the alleles at the SNPs in this block.\n",
    "        #...\n",
    "        #The last column is a string with the quality values (Sanger fastq format) for all the alleles covered by the fragment (concatenated for all blocks). \n",
    "        #For example, if a read/fragment covers SNPs 2,3 and 5 with the alleles 0, 1 and 0 respectively, then the input will be:\n",
    "        #2 read_id 2 01 5 0 AAC\n",
    "        #Here AAC is the string corresponding to the quality values at the three alleles. The encoding of 0/1 is arbitrary but following the VCF format, 0 is reference and 1 is alternate. \n",
    "        hairlist = hairline.strip().split()\n",
    "        self.blockcount = int(hairlist[0])     # number of blocks\n",
    "        self.read_id    = hairlist[1]          # read_id\n",
    "        self.blocks     = []\t\t       # an array of tuples corresponding to blocks\n",
    "        self.positions  = []\n",
    "        self.alleles    = []\n",
    "        self.haplotypes = dict()             # an array of {\"block_offset\":\"haplotype\"} \n",
    "                                           # after partitioning\n",
    "        for i in range(2, len(hairlist)-1, 2):\n",
    "            position = int(hairlist[i])\n",
    "            allele = hairlist[i+1]\n",
    "            block = zip(range(position, position+len(allele)), allele)\n",
    "            self.blocks.append(block)\n",
    "            self.positions.extend(range(position, position+len(allele)))\n",
    "            self.alleles.extend(allele)\n",
    "            self.qualities  = hairlist[-1]         # a matched arary of the qualities of allele calls\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<HapCutRead, read_id: %s>\" % str(self.read_id)\n",
    "    \n",
    "    def haplotype_fields(self):\n",
    "        haps = \";\".join([','.join([str(key), str(self.haplotypes[key])]) for key in self.haplotypes])\n",
    "        haptag = [(\"ZH\", haps), (\"ZB\", int(self.blockcount))]\n",
    "        return haptag\n",
    "\n",
    "class HairReader:\n",
    "\n",
    "    def __init__ (self, fn):\n",
    "        self.fn = fn\n",
    "        self.reads = []\n",
    "        with open (fn) as f:\n",
    "            for l in f:\n",
    "                self.reads.append(HapCutRead(l))\n",
    "        self.read_set = frozenset([x.read_id for x in self.reads])\n",
    "\n",
    "    def __repr__ (self):\n",
    "        return \"<HairReader, filename: %s>\" % self.fn\n",
    "    \n",
    "    def loc(self, read_id):\n",
    "        return next((x for x in self.reads if read_id == x.read_id))\n",
    "\n",
    "### FUNCTIONS ###\n",
    "\n",
    "'''\n",
    "\n",
    "tag_reads\n",
    "Ryan Neff\n",
    "7/5/2015\n",
    "\n",
    "Usage: Tags reads from a bam file corresponding to a particular haplotype, with haplotype\n",
    "definitions from HapCut, under the optional tag \"ZH\".\n",
    "\n",
    "Inputs:\n",
    "    bam_file (string)\n",
    "        The filename of a bam file on which the haplotype cuts were generated.\n",
    "    read_array (list of <HapCutRead>)\n",
    "        An array of HapCutRead objects that list which reads correspond to a particular haplotype block.\n",
    "    block_array (list of <BlockVariants>)\n",
    "        Of the format HapCutReader.blocks\n",
    "Outputs:\n",
    "    out_file (string)\n",
    "        The filename of the output (bam file).\n",
    "\n",
    "'''\n",
    "\n",
    "def tag_reads(bam_fp, hair_reader, block_reader, out_fp):\n",
    "    ''' tag_reads(bam_fp, hairs_file, hapcut_file, out_fp)'''\n",
    "    # let's group the reads by genomic position / block offset to speed up reading from the BAM file\n",
    "    for bamread in bam_fp.fetch():\n",
    "        if bamread.query_name in hair_reader.read_set:\n",
    "            read = hair_reader.loc(bamread.query_name)\n",
    "            read = greedy_partition(read, block_reader)\n",
    "            bamread.tags += read.haplotype_fields()      # add the haplotype information\n",
    "        out_fp.write(bamread)\n",
    "\n",
    "'''\n",
    "greedy_partition()\n",
    "Ryan Neff\n",
    "\n",
    "inputs:\n",
    "read\n",
    "    a HapCutRead object\n",
    "block_reader\n",
    "    of the type HapCutReader\n",
    "\n",
    "outputs:\n",
    "    the original read with haplotype information\n",
    "\n",
    "translate hairfile alleles into blockvar IDs\n",
    "get alleles in each read spanning blockvars\n",
    "determine alleles for the two blocks from blockvar\n",
    "partition read based on locally most probable alignment\n",
    "\n",
    "'''\n",
    "\n",
    "def greedy_partition(read, block_reader):\n",
    "    out_read = read\n",
    "    for readblock in out_read.blocks:\n",
    "        positions = [x[0] for x in readblock]\n",
    "        alleles = [x[1] for x in readblock]\n",
    "        allele_state = []\n",
    "        offset = positions[0]\n",
    "        hap = 0\n",
    "        block = block_reader.loc(offset) #retrieve block the read is in\n",
    "        if block == None:\n",
    "            continue\n",
    "        for ix, varpos in enumerate(positions):\n",
    "            blockvar = block.variant(varpos)\n",
    "            if blockvar.hap1 == alleles[ix]:\n",
    "                allele_state.append(-1)\n",
    "            elif blockvar.hap2 == alleles[ix]:\n",
    "                allele_state.append(1)\n",
    "            else:\n",
    "                sys.stderr.write(\"ERROR: read allele matched no haplotypes.\")\n",
    "                raise\n",
    "        if len(allele_state) < 1:\n",
    "            sys.stderr.write(\"Warning: no haplotype information in read.\")\n",
    "            hap = -1\n",
    "        if sum(allele_state) < 0:\n",
    "            hap = 1\n",
    "        elif sum(allele_state) > 0:\n",
    "            hap = 2\n",
    "        else:\n",
    "            hap = 0\n",
    "        out_read.haplotypes[block.offset] = hap\n",
    "    return out_read\n",
    "\n",
    "def interblock_stats(hair_reader, block_reader, out_stats=hairs_file + \".interblock_stats.tsv\"):\n",
    "    blockdist = []\n",
    "    lastChr = None\n",
    "    lastPos = None\n",
    "    lastBlock = None\n",
    "    lastReads = set()\n",
    "    for read in hair_reader.reads:\n",
    "        if read.haplotypes == dict():\n",
    "            read = greedy_partition(read, block_reader)\n",
    "    for block in block_reader.blocks:\n",
    "        if block.read_set == set():\n",
    "            block.addReadsToBlock(hair_reader.reads)\n",
    "        currBlock = block.offset\n",
    "        currChr = block.chrom\n",
    "        currPos = block.start\n",
    "        if lastBlock != None:\n",
    "            if lastChr == currChr:\n",
    "                interblock_reads = block.interblock_reads(lastBlock_obj)\n",
    "                row=[lastBlock, currBlock, currChr, lastPos, currPos, currPos-lastPos, \n",
    "                     len(lastBlock_obj.variant_ids), len(block.variant_ids), \n",
    "                     len(interblock_reads), \n",
    "                     lastBlock_obj.concordance(lastBlock_obj.interblock_reads(block)),\n",
    "                     block.concordance(block.interblock_reads(lastBlock_obj))]\n",
    "                blockdist.append(row)\n",
    "            else:\n",
    "                continue\n",
    "        lastBlock = currBlock\n",
    "        lastBlock_obj = block\n",
    "        lastChr = currChr\n",
    "        lastPos = block.end\n",
    "        lastReads = currReads\n",
    "    header = ['block1', 'block2', 'chrom', 'block1_end', 'block2_start', 'distance', 'block1_variants', 'block2_variants', \n",
    "              'interblock_reads', 'block1_interblock_concordance', 'block2_interblock_concordance']\n",
    "    info = pd.DataFrame(blockdist, columns=header)\n",
    "    info.to_csv(out_stats, sep=\"\\t\")\n",
    "\n",
    "# run the program if called from the command line\n",
    "#if __name__ == \"__main__\":\n",
    "#   main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hair_reader = HairReader(hairs_file)\n",
    "block_reader = HapCutReader(hapcut_file)\n",
    "for read in hair_reader.reads:\n",
    "    read = greedy_partition(read, block_reader)\n",
    "for block in block_reader.blocks:\n",
    "    block.addReadsToBlock(hair_reader.reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{58: {'hap1': (23, 0), 'hap2': (9, 0)}, 59: {'hap1': (23, 0), 'hap2': (9, 0)}, 60: {'hap1': (15, 0), 'hap2': (4, 0)}}\n",
      "\n",
      "{61: {'hap1': (7, 0), 'hap2': (7, 0)}, 63: {'hap1': (7, 0), 'hap2': (7, 0)}}\n",
      "\n",
      "{66: {'hap1': (15, 0), 'hap2': (15, 0)}, 67: {'hap1': (26, 0), 'hap2': (29, 0)}, 68: {'hap1': (18, 0), 'hap2': (18, 0)}, 69: {'hap1': (10, 0), 'hap2': (17, 0)}}\n",
      "\n",
      "{70: {'hap1': (17, 0), 'hap2': (17, 0)}, 71: {'hap1': (17, 0), 'hap2': (17, 0)}}\n",
      "\n",
      "{72: {'hap1': (4, 0), 'hap2': (3, 0)}, 74: {'hap1': (9, 0), 'hap2': (11, 0)}, 75: {'hap1': (5, 0), 'hap2': (8, 0)}}\n",
      "\n",
      "{80: {'hap1': (9, 0), 'hap2': (6, 0)}, 79: {'hap1': (9, 0), 'hap2': (6, 0)}}\n",
      "\n",
      "{83: {'hap1': (18, 0), 'hap2': (24, 0)}, 84: {'hap1': (18, 0), 'hap2': (24, 0)}}\n",
      "\n",
      "{88: {'hap1': (1, 0), 'hap2': (1, 0)}, 86: {'hap1': (9, 0), 'hap2': (13, 0)}, 87: {'hap1': (8, 0), 'hap2': (12, 0)}}\n",
      "\n",
      "{89: {'hap1': (8, 0), 'hap2': (4, 0)}, 90: {'hap1': (18, 0), 'hap2': (16, 0)}, 91: {'hap1': (11, 0), 'hap2': (11, 0)}, 92: {'hap1': (7, 0), 'hap2': (7, 0)}}\n",
      "\n",
      "{96: {'hap1': (9, 0), 'hap2': (7, 0)}, 97: {'hap1': (7, 0), 'hap2': (5, 0)}, 93: {'hap1': (19, 0), 'hap2': (16, 0)}, 94: {'hap1': (16, 0), 'hap2': (16, 0)}, 95: {'hap1': (7, 0), 'hap2': (2, 0)}}\n",
      "\n",
      "{99: {'hap1': (13, 1), 'hap2': (6, 0)}, 100: {'hap1': (14, 0), 'hap2': (6, 0)}}\n",
      "\n",
      "{104: {'hap1': (2, 0), 'hap2': (5, 0)}, 102: {'hap1': (16, 0), 'hap2': (13, 0)}, 103: {'hap1': (14, 0), 'hap2': (14, 0)}}\n",
      "\n",
      "{109: {'hap1': (12, 0), 'hap2': (25, 0)}, 110: {'hap1': (15, 0), 'hap2': (27, 0)}, 111: {'hap1': (4, 0), 'hap2': (7, 0)}}\n",
      "\n",
      "{112: {'hap1': (3, 0), 'hap2': (5, 0)}, 113: {'hap1': (3, 0), 'hap2': (5, 0)}}\n",
      "\n",
      "{116: {'hap1': (7, 0), 'hap2': (13, 0)}, 117: {'hap1': (8, 0), 'hap2': (16, 0)}, 118: {'hap1': (1, 0), 'hap2': (3, 0)}}\n",
      "\n",
      "{128: {'hap1': (1, 0), 'hap2': (6, 0)}, 129: {'hap1': (0, 0), 'hap2': (4, 0)}, 119: {'hap1': (14, 0), 'hap2': (9, 0)}, 120: {'hap1': (22, 0), 'hap2': (14, 0)}, 121: {'hap1': (22, 0), 'hap2': (16, 0)}, 122: {'hap1': (29, 0), 'hap2': (23, 0)}, 123: {'hap1': (22, 0), 'hap2': (24, 0)}, 124: {'hap1': (21, 0), 'hap2': (23, 0)}, 126: {'hap1': (17, 0), 'hap2': (19, 0)}, 127: {'hap1': (1, 0), 'hap2': (5, 0)}}\n",
      "\n",
      "{142: {'hap1': (4, 0), 'hap2': (4, 0)}, 143: {'hap1': (4, 0), 'hap2': (4, 0)}}\n",
      "\n",
      "{145: {'hap1': (19, 0), 'hap2': (16, 0)}, 147: {'hap1': (14, 0), 'hap2': (14, 0)}, 148: {'hap1': (29, 0), 'hap2': (19, 0)}, 149: {'hap1': (31, 0), 'hap2': (22, 0)}, 150: {'hap1': (21, 0), 'hap2': (30, 0)}, 151: {'hap1': (21, 0), 'hap2': (29, 0)}}\n",
      "\n",
      "{157: {'hap1': (1, 0), 'hap2': (2, 0)}, 159: {'hap1': (1, 0), 'hap2': (2, 0)}}\n",
      "\n",
      "{164: {'hap1': (14, 0), 'hap2': (8, 0)}, 165: {'hap1': (3, 0), 'hap2': (6, 0)}, 166: {'hap1': (29, 0), 'hap2': (25, 0)}, 167: {'hap1': (32, 0), 'hap2': (20, 0)}}\n",
      "\n",
      "{168: {'hap1': (7, 0), 'hap2': (0, 0)}, 170: {'hap1': (7, 0), 'hap2': (0, 0)}}\n",
      "\n",
      "{176: {'hap1': (3, 0), 'hap2': (5, 0)}, 174: {'hap1': (15, 0), 'hap2': (18, 0)}, 175: {'hap1': (16, 0), 'hap2': (19, 0)}}\n",
      "\n",
      "{177: {'hap1': (16, 0), 'hap2': (15, 0)}, 178: {'hap1': (16, 0), 'hap2': (15, 0)}}\n",
      "\n",
      "{180: {'hap1': (5, 0), 'hap2': (5, 0)}, 181: {'hap1': (5, 0), 'hap2': (5, 0)}}\n",
      "\n",
      "{182: {'hap1': (5, 0), 'hap2': (8, 0)}, 183: {'hap1': (5, 0), 'hap2': (8, 0)}}\n",
      "\n",
      "{186: {'hap1': (14, 0), 'hap2': (13, 0)}, 187: {'hap1': (14, 0), 'hap2': (13, 0)}}\n",
      "\n",
      "{188: {'hap1': (18, 0), 'hap2': (22, 0)}, 189: {'hap1': (18, 0), 'hap2': (25, 0)}, 190: {'hap1': (2, 0), 'hap2': (3, 0)}}\n",
      "\n",
      "{192: {'hap1': (9, 0), 'hap2': (10, 0)}, 191: {'hap1': (9, 0), 'hap2': (10, 0)}}\n",
      "\n",
      "{193: {'hap1': (1, 0), 'hap2': (0, 0)}, 194: {'hap1': (1, 0), 'hap2': (0, 0)}}\n",
      "\n",
      "{195: {'hap1': (4, 0), 'hap2': (2, 0)}, 197: {'hap1': (9, 0), 'hap2': (14, 0)}, 198: {'hap1': (5, 0), 'hap2': (12, 0)}}\n",
      "\n",
      "{200: {'hap1': (2, 0), 'hap2': (1, 0)}, 201: {'hap1': (19, 0), 'hap2': (15, 0)}, 202: {'hap1': (17, 0), 'hap2': (14, 0)}}\n",
      "\n",
      "{208: {'hap1': (19, 0), 'hap2': (18, 0)}, 210: {'hap1': (7, 0), 'hap2': (9, 0)}, 206: {'hap1': (12, 0), 'hap2': (11, 0)}, 207: {'hap1': (23, 0), 'hap2': (25, 0)}}\n",
      "\n",
      "{211: {'hap1': (11, 0), 'hap2': (6, 0)}, 212: {'hap1': (11, 0), 'hap2': (6, 0)}}\n",
      "\n",
      "{216: {'hap1': (20, 0), 'hap2': (21, 0)}, 217: {'hap1': (15, 0), 'hap2': (18, 0)}, 213: {'hap1': (14, 0), 'hap2': (13, 0)}, 214: {'hap1': (16, 0), 'hap2': (20, 0)}, 215: {'hap1': (20, 0), 'hap2': (20, 0)}}\n",
      "\n",
      "{224: {'hap1': (12, 0), 'hap2': (11, 0)}, 225: {'hap1': (5, 0), 'hap2': (7, 0)}, 226: {'hap1': (3, 0), 'hap2': (3, 0)}, 227: {'hap1': (11, 0), 'hap2': (3, 0)}, 228: {'hap1': (9, 0), 'hap2': (6, 0)}, 229: {'hap1': (17, 0), 'hap2': (7, 0)}, 219: {'hap1': (4, 0), 'hap2': (11, 0)}, 220: {'hap1': (8, 0), 'hap2': (14, 0)}, 221: {'hap1': (9, 0), 'hap2': (10, 0)}, 223: {'hap1': (10, 0), 'hap2': (12, 0)}}\n",
      "\n",
      "{233: {'hap1': (2, 0), 'hap2': (4, 0)}, 234: {'hap1': (2, 0), 'hap2': (4, 0)}}\n",
      "\n",
      "{235: {'hap1': (8, 1), 'hap2': (7, 0)}, 236: {'hap1': (9, 0), 'hap2': (7, 0)}}\n",
      "\n",
      "{243: {'hap1': (4, 0), 'hap2': (4, 0)}, 244: {'hap1': (4, 0), 'hap2': (4, 0)}}\n",
      "\n",
      "{245: {'hap1': (5, 0), 'hap2': (3, 0)}, 246: {'hap1': (17, 0), 'hap2': (20, 0)}, 247: {'hap1': (18, 0), 'hap2': (22, 0)}, 248: {'hap1': (11, 0), 'hap2': (24, 0)}, 249: {'hap1': (11, 0), 'hap2': (19, 0)}, 250: {'hap1': (21, 0), 'hap2': (20, 0)}, 251: {'hap1': (16, 0), 'hap2': (11, 0)}, 253: {'hap1': (17, 0), 'hap2': (16, 0)}, 254: {'hap1': (23, 0), 'hap2': (18, 0)}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in block_reader.blocks:\n",
    "    print block.concordance()\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interblock statistics**\n",
    "\n",
    "* length of block\n",
    "* variant count in block\n",
    "* total reads inside block\n",
    "* Informative reads inside block\n",
    "* Distance between blocks\n",
    "* Which blocks overlap\n",
    "* Interblock reads\n",
    "    * Read count between junctions\n",
    "    * 2x2 matrix with support for linking blocks if at junction\n",
    "* Total coverage between blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# interblock stats\n",
    "# imports\n",
    "import os, sys, getopt\n",
    "import pysam\n",
    "from itertools import groupby\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bam_fp = pysam.Samfile('/hpc/users/neffr01/2work/documents/test/NA18508.chr20.test.bam', 'rb')\n",
    "out_fp = pysam.AlignmentFile('/hpc/users/neffr01/2work/documents/test/NA18508.chr20.test_partition.bam', 'wb', template=bam_fp)\n",
    "hairs_file = '/hpc/users/neffr01/2work/documents/test/NA18508.chr20.test.hairs'\n",
    "hapcut_file = '/hpc/users/neffr01/2work/documents/test/NA18508.chr20.test.hapcut'\n",
    "\n",
    "hair_reader = HairReader(hairs_file)\n",
    "block_reader = HapCutReader(hapcut_file)\n",
    "\n",
    "tag_reads(bam_fp, hair_reader, block_reader, out_fp)\n",
    "interblock_stats(hair_reader, block_reader, hairs_file + \".interblock_stats.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#interblock distance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def interblock_stats(hair_reader, block_reader, out_stats=hairs_file + \".interblock_stats.tsv\"):\n",
    "    blockdist = []\n",
    "    lastChr = None\n",
    "    lastPos = None\n",
    "    lastBlock = None\n",
    "    lastReads = set()\n",
    "    for read in hair_reader.reads:\n",
    "        if read.haplotypes == dict():\n",
    "            read = greedy_partition(read, block_reader)\n",
    "    for block in block_reader.blocks:\n",
    "        if block.read_set == set():\n",
    "            block.addReadsToBlock(hair_reader.reads)\n",
    "        currBlock = block.offset\n",
    "        currChr = block.chrom\n",
    "        currPos = block.start\n",
    "        if lastBlock != None:\n",
    "            if lastChr == currChr:\n",
    "                interblock_reads = block.interblock_reads(lastBlock_obj)\n",
    "                row=[lastBlock, currBlock, currChr, lastPos, currPos, currPos-lastPos, \n",
    "                     len(lastBlock_obj.variant_ids), len(block.variant_ids), \n",
    "                     len(interblock_reads), \n",
    "                     lastBlock_obj.concordance(lastBlock_obj.interblock_reads(block)),\n",
    "                     block.concordance(block.interblock_reads(lastBlock_obj))]\n",
    "                blockdist.append(row)\n",
    "            else:\n",
    "                continue\n",
    "        lastBlock = currBlock\n",
    "        lastBlock_obj = block\n",
    "        lastChr = currChr\n",
    "        lastPos = block.end\n",
    "        lastReads = currReads\n",
    "    header = ['block1', 'block2', 'chrom', 'block1_end', 'block2_start', 'distance', 'block1_variants', 'block2_variants', \n",
    "              'interblock_reads', 'block1_interblock_concordance', 'block2_interblock_concordance']\n",
    "    info = pd.DataFrame(blockdist, columns=header)\n",
    "    info.to_csv(out_stats, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
